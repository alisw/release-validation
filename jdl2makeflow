#!/usr/bin/env python
from __future__ import print_function
import jinja2
import json
import jdl2makeflow_helpers
from jdl2makeflow_helpers import classad
import re, os, errno, sys
from shutil import copy2, rmtree
from argparse import ArgumentParser, REMAINDER
import subprocess
from collections import OrderedDict
from glob import glob

lib_path = os.path.abspath(os.path.dirname(jdl2makeflow_helpers.__file__))

# Tiers definition of stages for MC and for Reco. Jobs on each tier can be executed in parallel
stages_mc = [ [ "sim" ],
              [ "simResMerge", "simQaMerge" ],
              [ "simQaPlots" ] ]
stages_reco = [
                # CPass0
                [ "cpass0" ],
                [ "cpass0Merge", "cpass0ResMerge" ],
                [ "cpass0SpBins" ],
                [ "cpass0SpDistMaps" ],
                [ "cpass0SpCalib" ],
                # CPass1
                [ "cpass1" ],
                [ "cpass1Merge", "cpass1ResMerge" ],
                [ "cpass1QaMerge" ]
              ]
stages_flat = [ y for x in stages_mc+stages_reco for y in x  ]

ap = ArgumentParser()
ap.add_argument("--dryrun", "-n", dest="dry_run", default=None, action="store_true",
                help="Generate Makeflow scripts that do not actually run any job")
ap.add_argument("--makeflow-only", "-g", dest="makeflow_only", default=None, action="store_true",
                help="Generate Makeflow scripts only, do not run")
ap.add_argument("--workdir", "-w", dest="work_dir", default="work",
                help="Makeflow work directory (defaults to \"work\")")
ap.add_argument("--parse", dest="parse", default=False, action="store_true",
                help="Show all JDLs in JSON format before generating the Makeflow")
ap.add_argument("--parse-only", "-p", dest="parse_only", default=False, action="store_true",
                help="Only show JDLs in JSON format and quit")
ap.add_argument("--summary", "-s", dest="summary", default=False, action="store_true",
                help="Only show summary without creating any file and exit")
ap.add_argument("--start-at", "-t", dest="start_at", default=None,
                choices=stages_flat,
                help="Start at the defined stage, assuming the previous stages were successful")
ap.add_argument("--force", dest="force", default=False, action="store_true",
                help="Proceed even if working directory exists (you may lose files!)")
ap.add_argument("--remove", dest="remove", default=False, action="store_true",
                help="Force removal of working directory (you will lose files!)")
ap.add_argument("--graph", dest="graph", default=False, action="store_true",
                help="Create dependency graph as Makeflow.pdf in the workdir")
ap.add_argument("--run", "-r", dest="run", default=False, action="store_true",
                help="Automatically run workflow after generating the manifest")
ap.add_argument("jdl",
                help="AliEn JDL steering the jobs")
ap.add_argument("makeflow_opts", nargs=REMAINDER,
                help="Options for the makeflow command")
args = ap.parse_args()

j2env = jinja2.Environment()
j2env.filters["basename"] = lambda x: os.path.basename(x)
j2env.filters["job_head"] = \
  lambda stage, index=-1: \
    "{stage}{cond_index}.done: run{stage}.sh {stage_input}" \
      .format(stage=stage,
              index=0 if index < 0 else index,
              cond_index=("%04d"%index) if index > -1 else "",
              stage_input=" ".join([ x for x in jdls[stage].get("InputFile") if not re.search("^(/|[^ :]+:)", x) ]))
j2env.filters["job_cmd"] = \
  lambda stage, index=-1, extra_input=[], extra_output=[]: \
    "\t./run{stage}.sh {index} {stage}{cond_index}.done {input_file}" \
      .format(stage=stage,
              index=0 if index < 0 else index,
              cond_index=("%04d"%index) if index > -1 else "",
              input_file=jdls[stage]["InputFilesFromList"][index] if jdls[stage].get("InputFilesFromList", []) else "")

def preprocess_var(s, sh_mode):
  if isinstance(s, list):
    s = ",".join(s)
  if not re.search("#alien_counter(_[^#]+)?#|#alienfilename(/([^/#]+)/([^/#]*)/)?#", s):
    return s
  if sh_mode:
    # Shell mode: rely on script's alienfmt() shell function
    s = "$(alienfmt '%s')" % s
  else:
    # Jinja mode: use Jinja's builtin string formatting
    s = s.replace("#alien_counter#", "#alien_counter_i#")
    s = re.sub("#alien_counter_([^#]+)#", "{{'%\\1'|format(alien_counter)}}", s)
    s = s.replace("#alienfilename#", "{{input_files[alien_counter]|basename()}}")
    s = re.sub("#alienfilename/([^/#]+)/([^/#]*)/#", "{{input_files[alien_counter]|basename()|replace('\\1','\\2')}}", s)
  return s

def gen_runjob(output_file, jdl, dry_run):
  runjob = j2env.from_string("""#!/bin/bash -e
type zip unzip zipinfo
ulimit -c unlimited  # enable core dumps
{% for v in [ "LANG", "LANGUAGE", "LC_ALL", "LC_COLLATE", "LC_CTYPE", "LC_MESSAGES", \
              "LC_MONETARY", "LC_NUMERIC", "LC_TIME", "LC_ALL" ] -%}
export {{v}}=C
{% endfor -%}
JOBID=$1
JOBDIR=$(basename "$0")
JOBDIR=job-${JOBDIR%.*}-$(printf %04d $JOBID)
DONEFILE="$PWD/$2"
INPUTFILE="$3"
shift 3 || shift 2 || true

function alienfmt() {
  local ALIENFMT_STR="$1"
  local INPUTFILE_BASE=${INPUTFILE##*/}

  RE='^(.*)#alien_counter(_([^#]+))?#(.*)$'
  while [[ $ALIENFMT_STR =~ $RE ]]; do
    REPLACE=${BASH_REMATCH[3]}
    [[ $REPLACE ]] || REPLACE=i
    REPLACE=$(printf %${REPLACE} $JOBID)
    ALIENFMT_STR="${BASH_REMATCH[1]}${REPLACE}${BASH_REMATCH[4]}"
  done

  RE='^(.*)#alienfilename(/([^#/]+/[^#/]*)/)?#(.*)$'
  while [[ $ALIENFMT_STR =~ $RE ]]; do
    REPLACE=${BASH_REMATCH[3]}
    [[ $REPLACE ]] && REPLACE='${INPUTFILE_BASE//'$REPLACE'}' || REPLACE='$INPUTFILE_BASE'
    ALIENFMT_STR="${BASH_REMATCH[1]}${REPLACE}${BASH_REMATCH[4]}"
  done

  ALIENFMT_STR=$(eval printf -- \\""$ALIENFMT_STR"\\")
  echo "$ALIENFMT_STR"
}

rm -rf job-$JOBDIR
mkdir job-$JOBDIR
cd job-$JOBDIR
INPUT_LIST=({% for x in input_list -%}
            "{{x}}"
            {% endfor %})
OUTPUT_LIST=({% for x in output_list -%}
             "{{x}}"
             {% endfor %})
for INP in "${INPUT_LIST[@]}"; do
  PROTO=${INP%%://*}
  UNPACK=
  [[ $PROTO != $INP ]] || { PROTO=local; [[ $INP == /* ]] || INP="../$INP"; }
  [[ $PROTO == local || $PROTO == root ]] || { echo "Input protocol $PROTO not supported"; exit 1; }
  [[ $INP == *,unpack ]] && { UNPACK=1; INP=${INP//,unpack}; }
  for ((I=1; I<=5; I++)); do
    ERR=0
    echo "Transferring $INP to the input box (attempt $I/5)"
    case $PROTO in
      local) cp -v "$INP" . && break || ERR=$? ;;
      root)  xrdcp -f "$INP" . && break || ERR=$? ;;
    esac
  done
  [[ $ERR == 0 ]] || { echo "Error copying files into the input box"; exit 1; }
  if [[ $UNPACK ]]; then
    INP=${INP##*/}
    echo "Unpacking then removing $INP"
    case "$INP" in
      *.tar.bz2) UNPACK_CMD="tar xjvvf" ;;
      *.tar.gz)  UNPACK_CMD="tar xzvvf" ;;
      *.zip)     UNPACK_CMD="unzip"     ;;
      *) echo "Unsupported archive format: $INP"; exit 1; ;;
    esac
    $UNPACK_CMD "$INP" || ERR=$?
    rm -f "$INP"
  fi
  [[ $ERR == 0 ]] || { echo "Error unpacking files into the input box"; exit 1; }
done
echo "List of files in the working directory (recursive)"
find . -ls
echo "List of files ends here"
{% if env_cmd -%}
# Set custom environment (handling potential concurrency issues on our side)
LOCKDIR=/tmp/makeflow_env_lock
MAXWAIT=1000
sleep 0.1 &> /dev/null || MAXWAIT=100
for ((I=0; I<$MAXWAIT; I++)); do
  mkdir $LOCKDIR &> /dev/null && break || true
  sleep 0.1 || sleep 1
done
set +e
{{env_cmd}}
ENV_RV=$?
set -e
rmdir $LOCKDIR || true
[[ $ENV_RV == 0 ]] || exit 3
{% else -%}
eval $(/cvmfs/alice.cern.ch/bin/alienv printenv {{packages|join(",")}})
{% endif -%}
{{""}}
# Job environment
{% for x in environment -%}
export {{x}}="{{environment[x]|replace('"', '\\\\"')}}"
{% endfor -%}
{{""}}
echo "Output will be in $ALIEN_JDL_OUTPUTDIR"

# Execute command and ignore errors
ARGS="{{args}}"
PROG="{{executable}}"
{% if dry_run -%}
echo "Doing nothing: dry run" >> stdout.log
MAINERR=0
{% else -%}
type "$PROG" &> /dev/null || PROG="{{executable|basename()}}"
type "$PROG" &> /dev/null || PROG="$(find "$ALIDPG_ROOT" -name "{{executable|basename()}}" -print -quit || true)"
[[ "$PROG" ]] || PROG="./{{executable|basename()}}"
[[ -x "$PROG" ]] || PROG="bash $PROG"
MAINERR=0
{ env; echo + $PROG $ARGS; } >> stdout.log
$PROG $ARGS $*  > >(tee -a stdout.log{% if no_live_out %} &> /dev/null{% endif %}) \\
               2> >(tee -a stderr.log{% if no_live_out %} &> /dev/null{% endif %}) || MAINERR=$?
{% endif -%}
[[ $MAINERR != 0 ]] && echo "Exited with errors ($MAINERR)" || echo "Exited with no errors";
{ echo + $PROG $ARGS "exited with $MAINERR"; } >> stdout.log

# Produce validation report
touch validation_report.txt
grep -H -E 'std::bad_alloc|Segmentation violation|Segmentation fault|Bus error|floating point exception|Killed|busy flag cleared|Cannot Build the PAR Archive|\\*\\*\\* glibc detected \\*\\*\\*|E-AliCDBGrid::PutEntry:|F-AliCDBGrid::|E-TAlienFile::ReadBuffer: The remote \\(removed\\) file is not open' *.log *.nonesiste > validation_report.txt 2> /dev/null || true
for CORE in core*; do
  [[ -e $CORE ]] || continue
  echo "$CORE: core dumped"
done > validation_report.txt 2> /dev/null

# Make some files available to Makeflow directly too
[[ $ALIEN_JDL_TRANSFERTOMAKEFLOW ]] && cp -v $ALIEN_JDL_TRANSFERTOMAKEFLOW $(dirname $DONEFILE)/

# Compress output according to the output list
mkdir to_transfer
shopt -s extglob
for OUT in "${OUTPUT_LIST[@]}"; do
  ZIP=${OUT%%:*}
  FILES=${OUT#*:}
  [[ $ZIP == $OUT ]] && { echo "Not archiving $OUT"; mv -v $OUT to_transfer/ || true; continue; }
  [[ $FILES =~ \.root(,|$) ]] && ZIP_COMP="-0" || ZIP_COMP="-9"
  FILES=${FILES//,/ }
  echo $ZIP will contain $FILES
  ZIPERR=0
  zip $ZIP_COMP tmparchive.zip $FILES || ZIPERR=$?  # exitcode 12 is fine ==> "nothing to do"
  [[ $ZIPERR == 12 ]] && { echo "Zip $ZIP would be empty: not creating"; continue; } \\
                      || [[ $ZIPERR == 0 ]]
  rm -f $FILES  # same files cannot be in more than one archive
  mv tmparchive.zip to_transfer/$ZIP
done

# Copy files to destination (filesystem or xrootd)
PROTO=${ALIEN_JDL_OUTPUTDIR%%://*}
[[ $PROTO != $ALIEN_JDL_OUTPUTDIR ]] || { PROTO=local; mkdir -p $ALIEN_JDL_OUTPUTDIR; }
[[ $PROTO == local || $PROTO == root ]] || { echo "Output protocol $PROTO not supported;" exit 1; }
pushd to_transfer
  while read FILE; do
    for ((I=1; I<=5; I++)); do
      ERR=0
      echo "Transferring $FILE to $ALIEN_JDL_OUTPUTDIR (attempt $I/5)"
      case $PROTO in
        local) mkdir -p $(dirname "$ALIEN_JDL_OUTPUTDIR/$FILE"); cp -v $FILE $ALIEN_JDL_OUTPUTDIR/$FILE && break || ERR=$? ;;
        root)  xrdcp -f $FILE $ALIEN_JDL_OUTPUTDIR/$FILE && break || ERR=$? ;;
      esac
    done
  done < <(find . -type f | sed -e 's|^\./||')
popd

# Cleanup all
rm -rf *

# Signal success
echo Workflow execution completed: $PROG $ARGS exited with $MAINERR
touch $DONEFILE
""")
  with open(output_file, "w") as mf:
    mf.write(runjob.render(output_list = jdl["Output"],
                           input_list  = jdl["InputFile"],
                           packages    = jdl["Packages"],
                           env_cmd     = jdl.get("EnvironmentCommand", None),
                           no_live_out = jdl["NoLiveOutput"],
                           environment = jdl["Environment"],
                           dry_run     = dry_run,
                           executable  = jdl["Executable"],
                           args        = jdl["SplitArguments"]))
  os.chmod(output_file, int("755", 8))

def get_alien_xml(pattern, joba, jobb, input_files):
  axml = j2env.from_string(
"""<alien>
  <collection name="alien_collection.xml">
    {%- for alien_counter in range(joba,jobb+1) %}
    <event name="{{alien_counter}}">
      <file turl="***TURL***" type="f"/>
    </event>{% endfor %}
  </collection>
</alien>
""".replace("***TURL***", preprocess_var(pattern, sh_mode=False)))
  return axml.render(joba=joba, jobb=jobb, input_files=input_files)

def get_preprocessed_jdl(jdl_fn, override={}, append={}, delete=[]):
  jdl = classad.parse(open(jdl_fn).read(), ignore_errors=True)

  # Process overrides
  for k in override:
    jdl[k] = override[k]

  # Consider Arguments and SplitArguments equivalent
  if "Arguments" in jdl:
    jdl["SplitArguments"] = jdl["Arguments"]

  # Command-line arguments, supporting AliEn variables
  jdl["SplitArguments"] = preprocess_var(jdl["SplitArguments"], sh_mode=True)

  # Packages (filter out jemalloc)
  jdl["Packages"] = [ x for x in jdl["Packages"] if not "jemalloc" in x ]

  # Read input files list (optional)
  try:
    jdl["InputFilesFromList"] = [ x.strip() for x in
                                  open(os.path.basename(jdl.get("InputDataCollection", ""))).readlines()
                                  if not re.search("(^\s*#|^\s*$)", x) ]
  except Exception as e:
    jdl["InputFilesFromList"] = []

  # Job range ("Split" parameter)
  m = re.search("^([^:]+)(:([0-9]+)-([0-9]+))?$", jdl["Split"])
  if m.group(3) and m.group(4):
    jdl["JobRange"] = [ int(m.group(3)), int(m.group(4)) ]  # production
  else:
    jdl["JobRange"] = [ 0, max(0, len(jdl["InputFilesFromList"])-1) ]  # file

  # Remove @disk spec from output list
  jdl["Output"] = [ o.split("@", 1)[0] for o in jdl["Output"] ]

  # Input list: base path only (assume files are in the cwd or a "system" one), exclude OCDB
  jdl["InputFile"] = [ os.path.basename(x) if x.startswith("LF:") else x
                       for x in jdl.get("InputFile", []) ]
  jdl["InputFile"] = [ x for x in jdl["InputFile"] if not x.startswith("OCDB") ]

  # Process appends
  for k in append:
    if k in jdl:
      jdl[k] += append[k]

  # Determine whether this JDL comes from a MC or a Reco
  if jdl.get("Executable", "").endswith("aliroot_dpgsim.sh"):
    jdl["JobType"] = "MC"
  elif "/aliroot_dpg" in jdl.get("Executable", ""):
    jdl["JobType"] = "Reco"
  else:
    jdl["JobType"] = None

  # Job environment
  environment = {}
  for v in jdl.get("JDLVariables", []):
    environment["ALIEN_JDL_"+v.upper()] = preprocess_var(jdl.get(v, ""), sh_mode=True)  # with ALIEN_JDL_
  for v in jdl.get("ExtraVariables", []):
    environment[v] = preprocess_var(jdl.get(v, ""), sh_mode=True)  # exported as-is
  jdl["Environment"] = environment

  # Output requested not in zip files
  if jdl.get("DontArchive", None) == "1":
    jdl["DontArchive"] = True
    new_output = []
    for o in jdl["Output"]:
      if ":" in o:
        new_output += o.split(":", 1)[1].split(",")
      else:
        new_output.append(o)
    jdl["Output"] = new_output
  else:
    jdl["DontArchive"] = False

  # Save everything but no input files
  if jdl.get("SaveAll", None) == "1":
    jdl["SaveAll"] = True
    jdl["Output"] = [ "!(%s)" % "|".join(jdl["InputFile"]) ]
  else:
    jdl["SaveAll"] = False

  # No live output (convert to boolean)
  jdl["NoLiveOutput"] = jdl.get("NoLiveOutput", None) == "1"

  # Remove unneeded variables (cleanup)
  all_vars = jdl.keys()
  whitelist = [ "SplitArguments", "Executable", "Packages", "JobRange", "JobRange", "Output",
                "InputFile", "Environment", "NextStages", "OutputDir", "EnvironmentCommand",
                "SaveAll", "DontArchive", "NoLiveOutput", "InputFilesFromList", "JobType" ] + \
              list(override.keys()) + list(append.keys())
  for k in list(all_vars):
    if k in delete or k not in whitelist:
      del jdl[k]

  return jdl

# Store all input files for all jobs (dictionary of arrays): will be used by Jinja
all_inputs = {}
stages = None

# First tier of jobs: it also determines the job type (exit if unknown)
jdl = get_preprocessed_jdl(args.jdl)
if jdl["JobType"] == "MC":
  all_inputs["sim"] = jdl["InputFile"]
  stages = stages_mc
  steps = [ "MCPass" ]
elif jdl["JobType"] == "Reco":
  all_inputs["cpass0"] = jdl["InputFile"]
  stages = stages_reco
  steps = [ "CPass0" ] #, "CPass0_SPC", "CPass1", "PPass" ]
else:
  print("Cannot recognize job type, only MC and Reco are supported")
  exit(1)

# All JDLs are here. Dictionary whose labels are the stages defined above
jdls = OrderedDict()

# XMLs to generate (dict whose names are filenames)
xmls = {}

# Text lists to generate (dict whose names are filenames)
txts = {}

# Create all JDLs for MC
if jdl["JobType"] == "MC":

  # sim
  jdls["sim"] = jdl

  # simResMerge
  jdls["simResMerge"] = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "spcm_archive.zip:pyxsec*.root,AODQA.root,AliAOD*.root,FilterEvents_Trees*.root,*.stat*,EventStat_temp*.root,Residual*.root,TOFcalibTree.root,std*,fileinfo*.log@disk=2",
                             "validation_report.txt",
                             "core*" ],
                 "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)),
                                           "SpacePointCalibrationMerge", "001"),
                 "Executable": "merge.sh",
                 "InputXML": "residualtrees.xml",
                 "LPMCollectionEntity": "FilterEvents_Trees.root",
                 "NoGridConnect": "1",
                 "SplitArguments": "" },
      append={ "InputFile": [ "residualtrees.xml" ],
               "JDLVariables": [ "LPMCollectionEntity", "InputXML", "NoGridConnect" ] },
      delete=[ "JobRange", "InputFilesFromList" ])
  xmls["residualtrees.xml"] = \
    get_alien_xml(os.path.join(jdl["OutputDir"], "FilterEvents_Trees.root"),
                  jdl["JobRange"][0], jdl["JobRange"][1], jdl["InputFilesFromList"])

  # simQaMerge
  inputfile = "finalqa.txt" if jdl["DontArchive"] or jdl["SaveAll"] else "finalqa.xml"
  jdls["simQaMerge"] = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "QA_merge_log_archive.zip:std*,fileinfo*.log@disk=1",
                             "QA_merge_archive.zip:*QAresults*.root,EventStat_temp*.root,trending*.root,event_stat*.root,*.stat*@disk=3",
                             "validation_report.txt",
                             "core*" ],
                 "OutputDir": os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)),
                 "Executable": "aliprod_train_merge.sh",
                 "SplitArguments": "%s 5" % inputfile},  # 5 means "final stage"
      append={ "InputFile": [ inputfile, "aliprod_train_merge.sh" ] },
      delete=[ "JobRange" ])
  if inputfile.endswith(".xml"):
    xmls[inputfile] = \
      get_alien_xml(os.path.join(jdl["OutputDir"], "QA_archive.zip"),
                                 jdl["JobRange"][0], jdl["JobRange"][1],
                                 jdl["InputFilesFromList"])
  elif inputfile.endswith(".txt"):
    pattern = preprocess_var(jdl["OutputDir"], sh_mode=False)
    txts[inputfile] = \
      "\n".join([ j2env.from_string(pattern).render(alien_counter=ji, input_files=jdl["InputFilesFromList"])
                  for ji in range(jdl["JobRange"][0], jdl["JobRange"][1]+1) ]) + "\n"
    del pattern
  else:
    assert False, "inputfile should either end with .xml or .txt"
  del inputfile

  # simQaPlots
  inputs = [ "QAresults.root", "QAresults_merged.root", "QAresults_barrel.root",
             "QAresults_outer.root", "FilterEvents_Trees.root", "event_stat.root",
             "event_stat_barrel.root", "event_stat_outer.root" ] \
           if jdl["DontArchive"] or jdl["SaveAll"] else \
           [ "QA_merge_archive.zip" ]
  jdls["simQaPlots"] = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "qa_plots/*", "std*", "core*", "validation_report.txt" ],
                 "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "QAplots_passMC"),
                 "Executable": "qa_plots.sh",
                 "SplitArguments": " ".join([ os.path.join(preprocess_var(os.path.dirname(jdl["OutputDir"]), sh_mode=False), x) for x in inputs ]) },
      append={ "InputFile": [ "qa_plots.sh" ] },
      delete=[ "NextStages", "JobRange", "InputFilesFromList" ])
  del inputs

elif jdl["JobType"] == "Reco":

  # Run number in integer and string formats
  runNumberStr = os.path.basename(jdl["InputFilesFromList"][0])[2:11]
  runNumber = int(runNumberStr)

  # cpass0
  jdls["cpass0"] = jdl

  # cpass0Merge
  jdls["cpass0Merge"] = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "log_archive.zip:*.log@disk=2",
                             "ocdb_archive.zip:CalibObjects.root,meanITSVertex.root,fitITSVertex.root,cpassStat.root@disk=4",
                             "OCDB.tar.bz2",
                             "validation_report.txt",
                             "core*" ],
                 "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "OCDB"),
                 "Executable": "aliroot_dpgMergeMakeOCDB_CPass0.sh",
                 "Arguments": "calibobjects.xml %d local://./OCDB fileAccessMethod=copyXMLcollection makeOCDB=1 calibObjectsFileName=CalibObjects.root" % runNumber,
                 "CreateOCDBArchive": "1"
               },
      append={ "InputFile"   : [ "calibobjects.xml" ],
               "JDLVariables": [ "CreateOCDBArchive" ] },
      delete=[ "JobRange", "InputFilesFromList" ])
  xmls["calibobjects.xml"] = \
    get_alien_xml(os.path.join(jdl["OutputDir"], "CalibObjects.root"),
                               jdl["JobRange"][0], jdl["JobRange"][1], jdl["InputFilesFromList"])

  # cpass0ResMerge
  jdls["cpass0ResMerge"] = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "spcm_archive.zip:pyxsec*.root,AODQA.root,AliAOD*.root,FilterEvents_Trees*.root,*.stat*,EventStat_temp*.root,Residual*.root,TOFcalibTree.root,std*,fileinfo*.log@disk=2",
                             "validation_report.txt",
                             "core*" ],
                 "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)),
                                           "ResidualMerge", "001"),
                 "Executable": "merge.sh",
                 "InputXML": "residualtrees.xml",
                 "LPMCollectionEntity": "ResidualTrees.root",
                 "NoGridConnect": "1",
                 "SplitArguments": "" },
      append={ "InputFile": [ "residualtrees.xml" ],
               "JDLVariables": [ "LPMCollectionEntity", "InputXML", "NoGridConnect" ] },
      delete=[ "JobRange", "InputFilesFromList" ])
  xmls["residualtrees.xml"] = \
    get_alien_xml(os.path.join(jdl["OutputDir"], "ResidualTrees.root"),
                               jdl["JobRange"][0], jdl["JobRange"][1], jdl["InputFilesFromList"])

  # cpass0SpBins
  jdls["cpass0SpBins"] = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "log_archive.zip:*.log,*.list@disk=2",
                             "root_archive.zip:*.root@disk=2",
                             "validation_report.txt",
                             "core*" ],
                 "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "ResidualMerge", "TPCSPCalibration"),
                 "Executable": "aliroot_procVDTime.sh",
                 "Arguments": "tpcSPCalibration.xml %s" % runNumberStr,
                 "targetOCDBDir": "local://./OCDB",  # output for OCDB files, it's "same" (raw://) on the Grid (OCDB is prepopulated from tarball)
                 "useTOFBC": "false",
                 "transferToMakeflow": "timeBins.log"
               },
      append={ "InputFile": [ "tpcSPCalibration.xml",
                              os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "OCDB", "OCDB.tar.bz2,unpack") ],
               "JDLVariables": [ "targetOCDBDir", "useTOFBC", "transferToMakeflow" ] },
      delete=[ "NextStages", "JobRange", "InputFilesFromList" ])
  xmls["tpcSPCalibration.xml"] = \
    get_alien_xml(
      os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)),
                   "ResidualMerge", "001", "ResidualTrees.root"), 0, 0, [])

else:
  assert False, "should not happen: jdl[\"JobType\"] should be handled at this point"


#
## Reco CPass0: TPCSPVDTime
#jdl_reco_tpcspvdtime = {}
#if "TPCSPVDTime" in jdl.get("NextStages", []):
#  runNumberStr = os.path.basename(jdl["InputFilesFromList"][0])[2:11]
#  jdl_reco_tpcspvdtime = \
#    get_preprocessed_jdl(args.jdl,
#      override={ "Output": [ "log_archive.zip:*.log,*.list@disk=2",
#                             "root_archive.zip:*.root@disk=2",
#                             "validation_report.txt",
#                             "core*" ],
#                 "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "ResidualMerge", "TPCSPCalibration"),
#                 "Executable": "aliroot_procVDTime.sh",
#                 "Arguments": "tpcSPCalibration.xml %s" % runNumberStr,
#                 "targetOCDBDir": "local://./OCDB",  # output for OCDB files, it's "same" (raw://) on the Grid (OCDB is prepopulated from tarball)
#                 "useTOFBC": "false"
#               },
#      append={ "InputFile": [ "tpcSPCalibration.xml",
#                              os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "OCDB", "OCDB.tar.bz2,unpack") ],
#               "JDLVariables": [ "targetOCDBDir", "useTOFBC" ] },
#      delete=[ "NextStages", "JobRange", "InputFilesFromList" ])
#  all_inputs["TPCSPVDTime"] = jdl_reco_tpcspvdtime["InputFile"]
#  xml_tpcspcalibration = get_alien_xml(
#    os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "ResidualMerge", "001", "ResidualTrees.root"),
#    0, 0, [])
#
## Reco CPass1
#jdl_reco_cpass1 = {}
#if "CPass1" in jdl.get("NextStages", []):
#  jdl_reco_cpass1 = \
#    get_preprocessed_jdl(args.jdl,
#      override={ "Output": [ "log_archive.zip:qa*.log,calib.log,filtering.log,mergeQA_outer.log,mergeQA_barrel.log,rec.log,stderr.log,stdout.log@disk=1",
#                             "root_archive.zip:AliESDs_Barrel.root,CalibObjects.root,*.ESD.tag.root,TOFcalibTree.root,T0AnalysisTree.root@disk=2",
#                             "QA_archive.zip:QAresults_barrel.root,QAresults_outer.root,*.stat.qa*@disk=2",
#                             "EventStat_temp*.root@disk=2",
#                             "ResidualTrees.root@disk=2",
#                             "FilterEvents_Trees.root@disk=2",
#                             "validation_report.txt",
#                             "core*" ],
#                 "OutputDir": jdl["OutputDir"].replace("/cpass0_pass1/", "/cpass1_pass1/"),
#                 "Executable": "aliroot_dpgCPass1.sh",
#                 "LPMPass": "1",
#                 "LPMPassName": "cpass1_pass1",
#                 "LPMRAWPassID": "1",
#                 "LPMCPassMode": "1"
#               },
#      append={ "InputFile"   : [ os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "OCDB", "OCDB.tar.bz2,unpack") ] },
#      delete=[ "NextStages" ])
#  all_inputs["CPass1"] = jdl_reco_cpass1["InputFile"]
#
## Reco CPass1: merge and make OCDB
#jdl_reco_mergemakeocdbcpass1 = {}
#if "MergeMakeOCDB_CPass1" in jdl.get("NextStages", []):
#  runNumber = int(os.path.basename(jdl["InputFilesFromList"][0])[2:11])
#  jdl_reco_mergemakeocdbcpass1 = \
#    get_preprocessed_jdl(args.jdl,
#      override={ "Output": [ "log_archive.zip:*.log@disk=2",
#                             "ocdb_archive.zip:CalibObjects.root,meanITSVertex.root,fitITSVertex.root,cpassStat.root@disk=4",
#                             "OCDB.tar.bz2",
#                             "validation_report.txt",
#                             "core*" ],
#                 "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdl_reco_cpass1["OutputDir"], sh_mode=False)), "OCDB"),
#                 "Executable": "aliroot_dpgMergeMakeOCDB_CPass1.sh",
#                 "Arguments": "calibobjects_cpass1.xml %d local://./OCDB fileAccessMethod=copyXMLcollection makeOCDB=1 calibObjectsFileName=CalibObjects.root" % runNumber,
#                 "CreateOCDBArchive": "1"
#               },
#      append={ "InputFile"   : [ "calibobjects_cpass1.xml",
#                                 os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "OCDB", "OCDB.tar.bz2,unpack") ],
#               "JDLVariables": [ "CreateOCDBArchive" ] },
#      delete=[ "NextStages", "JobRange", "InputFilesFromList" ])
#  all_inputs["MergeMakeOCDB_CPass1"] = jdl_reco_mergemakeocdbcpass1["InputFile"]
#  xml_calib_cpass1 = get_alien_xml(os.path.join(jdl_reco_cpass1["OutputDir"], "CalibObjects.root"),
#                                   jdl_reco_cpass1["JobRange"][0], jdl_reco_cpass1["JobRange"][1],
#                                   jdl_reco_cpass1["InputFilesFromList"])
#
##if "TPCSPProcTBinMap" in jdl.get("NextStages", []):
##  #runNumber = int(os.path.basename(jdl["InputFilesFromList"][0])[2:11])
##  jdl_reco_tpcspproctbinmap = \
##    get_preprocessed_jdl(args.jdl,
##      override={ "Output": [ "log_archive.zip:*.log@disk=2",
##                             "root_archive.zip:alitpcdcalibres.root,voxelResTree.root@disk=2" ],
##                 "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "ResidualMerge", "TPCSPCalibration"),
##                 "Executable": "aliroot_procTBinMap.sh",
##                 "Arguments": ""
##               },
##      append={ "InputFile"   : [ "calibobjects.xml" ],
##               "JDLVariables": [ "CreateOCDBArchive" ] },
##      delete=[ "NextStages", "JobRange" ])
##  all_inputs["MergeMakeOCDB_CPass0"] = jdl_reco_mergemakeocdbcpass0["InputFile"]
##  xml_reco = get_alien_xml(os.path.join(jdl["OutputDir"], "CalibObjects.root"),
##                           jdl["JobRange"][0], jdl["JobRange"][1], jdl["InputFilesFromList"])
#
## Second tier of jobs: merging of filtered trees
#jdl_mft = {}
#if "MergeFilteredTrees" in jdl.get("NextStages", []):
#  filtered_trees_fn = "ResidualTrees.root" if "MergeMakeOCDB_CPass0" in jdl.get("NextStages", []) else "FilterEvents_Trees.root"
#  jdl_mft = get_preprocessed_jdl(args.jdl,
#              override={ "Output": [ "spcm_archive.zip:pyxsec*.root,AODQA.root,AliAOD*.root,FilterEvents_Trees*.root,*.stat*,EventStat_temp*.root,Residual*.root,TOFcalibTree.root,std*,fileinfo*.log@disk=2",
#                                     "validation_report.txt",
#                                     "core*" ],
#                         "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)),
#                                                   "ResidualMerge" if "MergeMakeOCDB_CPass0" else "SpacePointCalibrationMerge",
#                                                   "001"),
#                         "Executable": "merge.sh",
#                         "InputXML": "residualtrees.xml",
#                         "LPMCollectionEntity": filtered_trees_fn,
#                         "NoGridConnect": "1",
#                         "SplitArguments": "" },
#              append={ "InputFile": [ "residualtrees.xml" ],
#                       "JDLVariables": [ "LPMCollectionEntity", "InputXML", "NoGridConnect" ] },
#              delete=[ "NextStages", "JobRange", "InputFilesFromList" ])
#  xml_mft = get_alien_xml(os.path.join(jdl["OutputDir"], filtered_trees_fn),
#                          jdl["JobRange"][0], jdl["JobRange"][1], jdl["InputFilesFromList"])
#  all_inputs["MergeFilteredTrees"] = jdl_mft["InputFile"]
#
## Reco CPass1: merging of filtered trees
#jdl_reco_cpass1mft = {}
#if "MergeFilteredTrees_CPass1" in jdl.get("NextStages", []):
#  filtered_trees_fn = "FilterEvents_Trees.root"
#  jdl_reco_cpass1mft = get_preprocessed_jdl(args.jdl,
#              override={ "Output": [ "spcm_archive.zip:pyxsec*.root,AODQA.root,AliAOD*.root,FilterEvents_Trees*.root,*.stat*,EventStat_temp*.root,Residual*.root,TOFcalibTree.root,std*,fileinfo*.log@disk=2",
#                                     "validation_report.txt",
#                                     "core*" ],
#                         "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdl_reco_cpass1["OutputDir"], sh_mode=False)),
#                                                   "MergedTrees",
#                                                   "001"),
#                         "Executable": "merge.sh",
#                         "InputXML": "residualtrees_cpass1.xml",
#                         "LPMCollectionEntity": filtered_trees_fn,
#                         "NoGridConnect": "1",
#                         "SplitArguments": "" },
#              append={ "InputFile": [ "residualtrees_cpass1.xml" ],
#                       "JDLVariables": [ "LPMCollectionEntity", "InputXML", "NoGridConnect" ] },
#              delete=[ "NextStages", "JobRange", "InputFilesFromList" ])
#  xml_cpass1mft = get_alien_xml(os.path.join(jdl_reco_cpass1["OutputDir"], filtered_trees_fn),
#                                jdl_reco_cpass1["JobRange"][0], jdl_reco_cpass1["JobRange"][1],
#                                jdl_reco_cpass1["InputFilesFromList"])
#  all_inputs["MergeFilteredTrees_CPass1"] = jdl_reco_cpass1mft["InputFile"]
#
## Second tier of jobs: FinalQA
#jdl_fqa = {}
#if "FinalQA" in jdl.get("NextStages", []):
#  isMC = not "CPass1" in jdl.get("NextStages", [])
#  jdl_base = jdl if isMC else jdl_reco_cpass1  # do it for CPass1 on reconstruction
#  inputfile_fqa = "finalqa.txt" if jdl["DontArchive"] or jdl["SaveAll"] else "finalqa.xml"
#  jdl_fqa = get_preprocessed_jdl(args.jdl,
#              override={ "Output": [ "QA_merge_log_archive.zip:std*,fileinfo*.log@disk=1",
#                                     "QA_merge_archive.zip:*QAresults*.root,EventStat_temp*.root,trending*.root,event_stat*.root,*.stat*@disk=3",
#                                     "validation_report.txt",
#                                     "core*" ],
#                         "OutputDir": os.path.dirname(preprocess_var(jdl_base["OutputDir"], sh_mode=False)),
#                         "Executable": "aliprod_train_merge.sh" if isMC else "train_merge.sh",
#                         "SplitArguments": "%s 5" % inputfile_fqa},  # 5 means "final stage"
#              append={ "InputFile": [ inputfile_fqa ] + ([ "aliprod_train_merge.sh" ] if isMC else []) },
#              delete=[ "NextStages", "JobRange" ])
#  if inputfile_fqa.endswith(".xml"):
#    inputlist_fqa = get_alien_xml(os.path.join(jdl_base["OutputDir"], "QA_archive.zip"),
#                                  jdl_base["JobRange"][0], jdl_base["JobRange"][1],
#                                  jdl_base["InputFilesFromList"])
#  else:
#    pattern = preprocess_var(jdl_base["OutputDir"], sh_mode=False)
#    inputlist_fqa = "\n".join([ j2env.from_string(pattern).render(alien_counter=ji, input_files=jdl_base["InputFilesFromList"])
#                                for ji in range(jdl_base["JobRange"][0], jdl_base["JobRange"][1]+1) ]) + "\n"
#  all_inputs["FinalQA"] = jdl_fqa["InputFile"]
#  qap_inputs = [ "QAresults.root", "QAresults_merged.root", "QAresults_barrel.root",
#                 "QAresults_outer.root", "FilterEvents_Trees.root", "event_stat.root",
#                 "event_stat_barrel.root", "event_stat_outer.root" ] \
#                if jdl["DontArchive"] or jdl["SaveAll"] else \
#               [ "QA_merge_archive.zip" ]
#  jdl_qap = get_preprocessed_jdl(args.jdl,
#              override={ "Output": [ "qa_plots/*", "std*", "core*", "validation_report.txt" ],
#                         "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdl_base["OutputDir"], sh_mode=False)), "QAplots_passMC" if isMC else "QAPlots_CPass1"),
#                         "Executable": "qa_plots.sh",
#                         "SplitArguments": " ".join([ os.path.join(preprocess_var(os.path.dirname(jdl_base["OutputDir"]), sh_mode=False), x) for x in qap_inputs ]) },
#              append={ "InputFile": [ "qa_plots.sh" ] },
#              delete=[ "NextStages", "JobRange", "InputFilesFromList" ])
#  all_inputs["QAPlots"] = jdl_qap["InputFile"]

if args.parse or args.parse_only:
  for j in jdls:
    print("# JSONized JDL: %s" % j)
    print(json.dumps(jdls[j], indent=2))
    print("# End of %s" % j)
    print()
  if args.parse_only:
    exit(0)

# Summary
print("""Running the workflow with the following configuration:

Packages:
%(packages)s

%(numjobs)d total jobs, with job IDs from %(joba)d to %(jobb)d (included), will execute the command:
%(command)s

Required files (must be in the current directory, will be made available to each job):
%(reqd)s

Input files:
%(input)s

Output files (archives with content listed):
%(output)s

Output directory:
%(outdir)s

Environment variables available to the jobs:
%(env)s
""" % { "packages" : " * "+"\n * ".join(jdl["Packages"]) if not "EnvironmentCommand" in jdl else \
                     " * Custom environment command: " + jdl["EnvironmentCommand"],
        "numjobs"  : jdl["JobRange"][1]-jdl["JobRange"][0]+1,
        "joba"     : jdl["JobRange"][0],
        "jobb"     : jdl["JobRange"][1],
        "command"  : jdl["Executable"] + " " + jdl["SplitArguments"],
        "reqd"     : " * "+"\n * ".join(jdl["InputFile"]),
        "input"    : " * "+"\n * ".join(jdl["InputFilesFromList"]),
        "output"   : " * "+"\n * ".join([ " ==> ".join(x.split(":", 1)) for x in jdl["Output"] ]),
        "outdir"   : jdl["OutputDir"],
        "env"      : " * "+"\n * ".join([ "%s ==> %s"%(x,jdl["Environment"][x]) for x in jdl["Environment"]])

})
sys.stdout.flush()
if args.summary:
  exit(0)

# Create working directory. From now on we write to disk, not before
if args.remove:
  try:
    rmtree(args.work_dir)
  except:
    pass
try:
  os.mkdir(args.work_dir)
except OSError as e:
  if e.errno == errno.EEXIST:
    if not args.force:
      print("Cannot create output directory \"%s\": remove existing one first" % args.work_dir)
      exit(1)
    else:
      print("WARNING: output directory \"%s\" exists already" % args.work_dir)
  else:
    print("Cannot create output directory \"%s\": %s" % (args.workdir, e))
    exit(1)

# Remove all done files found
if args.force:
  print("Removing all existing state files")
  for f in glob(os.path.join(args.work_dir, "*.done")):
    os.remove(f)
  for f in glob(os.path.join(args.work_dir, "*.makeflowlog")):
    os.remove(f)

# Create all XML and text files
for x in xmls:
  with open(os.path.join(args.work_dir, x), "w") as f:
    print("Writing XML collection %s" % x)
    f.write(xmls[x])
for t in txts:
  with open(os.path.join(args.work_dir, t), "w") as f:
    print("Writing text collection %s" % t)
    f.write(txts[t])

# Generate all run*.sh scripts (the job wrappers from Jinja)
for j in jdls:
  print("Writing job wrapper run%s.sh" % j)
  gen_runjob(os.path.join(args.work_dir, "run%s.sh" % j), jdls[j], args.dry_run)

# Copy all input files once, when appropriate, to the work directory. Files accessed from a remote
# location will not be copied
for f in set([ i for j in jdls for k in jdls[j] for i in jdls[j].get("InputFile", []) ]):
  dest = os.path.join(args.work_dir, f)
  if f in xmls or f in txts or re.search("^(/|[^ :]+:)", f):
    continue  # do not copy this file (was generated, or from a remote location)
  print("Copying %s to the work directory" % f)
  try:
    os.remove(dest)
  except:
    pass
  try:
    copy2(f, dest)  # try from current dir first
  except:
    try:
      copy2(os.path.join(lib_path, f), dest)  # fallback on installation path
    except IOError:
      print("Cannot copy input file \"%s\", please make it available in the "
            "current directory (%s) or remove it from the JDL" % (f, os.getcwd()))
      exit(1)

# Touch donefiles to pretend we have completed the earlier stages
if args.start_at:
  for tier in stages:
    for to_touch in [ x for x in tier if x != args.start_at ]:
      print("Pretending we have executed step %s" % to_touch)
      jr = jdls[to_touch].get("JobRange")
      if jr:
        for jobindex in range(jr[0], jr[1]+1):
          open(os.path.join(args.work_dir, "%s%04d.done" % (to_touch, jobindex)), "w").close()
      else:
        open(os.path.join(args.work_dir, "%s.done" % to_touch), "w").close()
    if args.start_at in tier:
      break

# Run the workflow with additional checks: every Makeflow manifest is generated at each step
if args.run:
  for step in steps:
    print("Generating Makeflow_%s" % step)

#{% if step == "CPass1" %}
## CPass1: second tier: parallel CPass1 reconstruction jobs
#{% for jobindex in range(joba,jobb) -%}
#cpass1reco{{ "%04d"|format(jobindex) }}.done: runcpass1.sh {{input_list["CPass1"]|join(" ")}} tpcspvdtime.done
#	./runcpass1.sh {{jobindex}} cpass1reco{{"%04d"|format(jobindex)}}.done{% if input_files|length > 1 %} {{input_files[jobindex-joba]}}{% endif %}
#{% endfor -%}
#
## CPass1: merge calibration objects and upload OCDB
#mergemakeocdbcpass1.done: runmergemakeocdbcpass1.sh {{input_list["MergeMakeOCDB_CPass1"]|join(" ")}}{% for jobindex in range(joba,jobb) %} cpass1reco{{"%04d"|format(jobindex)}}.done{% endfor %}
#	./runmergemakeocdbcpass1.sh 0 mergemakeocdbcpass1.done
#
## CPass1: merge filtered trees
#mergefilteredtreescpass1.done: runcpass1mft.sh {{input_list["MergeFilteredTrees_CPass1"]|join(" ")}}{% for jobindex in range(joba,jobb) %} cpass1reco{{"%04d"|format(jobindex)}}.done{% endfor %}
#	./runcpass1mft.sh 0 mergefilteredtreescpass1.done
#
## CPass1: merge QA results
#finalqa.done: runfinalqa.sh {{input_list["FinalQA"]|join(" ")}}{% for jobindex in range(joba,jobb) %} {% if "CPass1" in next_stages -%}cpass1reco{% else %}job{% endif %}{{"%04d"|format(jobindex)}}.done{% endfor %}
#	./runfinalqa.sh 0 finalqa.done
#
## CPass1: create QA plots
#qaplots.done: runqaplots.sh finalqa.done {{input_list["QAPlots"]|join(" ")}}
#	./runqaplots.sh 0 qaplots.done
#{% endif %}
#""")

    makeflow = j2env.from_string("""
# Makeflow_{{step}} -- automatically generated

{% if step == "MCPass" %}
# sim
{% for jobindex in range(joba,jobb) -%}
{{ "sim"|job_head(index=jobindex) }}
{{ "sim"|job_cmd(index=jobindex) }}
{% endfor %}

# simResMerge
{{ "simResMerge"|job_head }}{% for j in range(joba,jobb) %}{{ " sim%04d.done"|format(j) }}{% endfor %}
{{ "simResMerge"|job_cmd }}

# simQaMerge
{{ "simQaMerge"|job_head }}{% for j in range(joba,jobb) %}{{ " sim%04d.done"|format(j) }}{% endfor %}
{{ "simQaMerge"|job_cmd }}

# simQaPlots
{{ "simQaPlots"|job_head }} simQaMerge.done
{{ "simQaPlots"|job_cmd }}
{% endif %}

{% if step == "CPass0" %}
# cpass0
{% for jobindex in range(joba,jobb) -%}
{{ "cpass0"|job_head(index=jobindex) }}
{{ "cpass0"|job_cmd(index=jobindex) }}
{% endfor %}

# cpass0Merge
{{ "cpass0Merge"|job_head }}{% for j in range(joba,jobb) %}{{ " cpass0%04d.done"|format(j) }}{% endfor %}
{{ "cpass0Merge"|job_cmd }}

# cpass0ResMerge
{{ "cpass0ResMerge"|job_head }}{% for j in range(joba,jobb) %}{{ " cpass0%04d.done"|format(j) }}{% endfor %}
{{ "cpass0ResMerge"|job_cmd }}

# cpass0SpBins
timeBins.log {{ "cpass0SpBins"|job_head }} cpass0Merge.done cpass0ResMerge.done
{{ "cpass0SpBins"|job_cmd }}

{% endif %}

""")
    with open(os.path.join(args.work_dir, "Makeflow_"+step), "w") as mf:
      mf.write(makeflow.render(jdls=jdls,
                               step=step,
                               joba= jdl["JobRange"][0],
                               jobb= jdl["JobRange"][1]+1))

    if args.graph:
      print("Generating graph for step %s" % step)
      wd = os.getcwd()
      os.chdir(args.work_dir)
      try:
        subprocess.check_call("makeflow_viz Makeflow_%s | dot -T pdf -o Makeflow_%s.pdf" % (step, step),
                              shell=True)
      except:
        print("WARNING: cannot generate workflow graph for %s" % step)
      os.chdir(wd)

    # Run Makeflow in dryrun mode to pretend we have completed the earlier stages
    if args.start_at:
      print("Dry-running Makeflow on step %s" % step)
      wd = os.getcwd()
      os.chdir(args.work_dir)
      devnull = open(os.devnull)
      subprocess.check_call([ "makeflow", "-T", "dryrun", "Makeflow_"+step ],
                            stdout=devnull, stderr=devnull)
      os.chdir(wd)

    print("Executing step %s" % step)
    wd = os.getcwd()
    os.chdir(args.work_dir)
    try:
      subprocess.check_call( ["time"] + (["echo", "+"]if args.makeflow_only else []) + \
                             [ "makeflow", "Makeflow_"+step ] + args.makeflow_opts)
    except subprocess.CalledProcessError as e:
      rv = e.returncode if e.returncode > 0 else 128-e.returncode
      print("Step %s finished with an error: %d" % (step, rv))
      exit(rv)
    os.chdir(wd)
    print("Step %s finished successfully" % step)
  exit(0)
