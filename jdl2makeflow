#!/usr/bin/env python
from __future__ import print_function
import jinja2
import json
import jdl2makeflow_helpers
from jdl2makeflow_helpers import classad
import re, os, errno, sys
from shutil import copy2, rmtree
from argparse import ArgumentParser, REMAINDER
import subprocess
from collections import OrderedDict
from glob import glob

lib_path = os.path.abspath(os.path.dirname(jdl2makeflow_helpers.__file__))

# Tiers definition of stages for MC and for Reco. Jobs on each tier can be executed in parallel
stages_mc = [ [ "sim" ],
              [ "simResMerge", "simQaMerge" ],
              [ "simQaPlots" ],
              [ "mergeValidationReportsMC" ] ]
stages_reco = [
                # CPass0
                [ "cpass0" ],
                [ "cpass0Merge", "cpass0ResMerge" ],
                [ "cpass0SpBins" ],
                [ "cpass0SpDistMaps" ],
                [ "cpass0SpCalib" ],
                # CPass1
                [ "cpass1" ],
                [ "cpass1Merge", "cpass1ResMerge" ],
                [ "cpass1QaMerge" ],
                [ "cpass1QaPlots" ],
                # PPass
                [ "ppass" ],
                [ "ppassQaMerge", "ppassResMerge" ],
                [ "ppassQaPlots" ],
                # Validation reports
                [ "mergeValidationReportsReco" ]
              ]
stages_flat = [ y for x in stages_mc+stages_reco for y in x  ]

ap = ArgumentParser()
ap.add_argument("--dryrun", "-n", dest="dry_run", default=None, action="store_true",
                help="Generate Makeflow scripts that do not actually run any job")
ap.add_argument("--workdir", "-w", dest="work_dir", default="work",
                help="Makeflow work directory (defaults to \"work\")")
ap.add_argument("--parse", dest="parse", default=False, action="store_true",
                help="Show all JDLs in JSON format before generating the Makeflow")
ap.add_argument("--parse-only", "-p", dest="parse_only", default=False, action="store_true",
                help="Only show JDLs in JSON format and quit")
ap.add_argument("--summary", "-s", dest="summary", default=False, action="store_true",
                help="Only show summary without creating any file and exit")
ap.add_argument("--start-at", "-t", dest="start_at", default=None,
                choices=stages_flat,
                help="Start at the defined stage, assuming the previous stages were successful")
ap.add_argument("--force", dest="force", default=False, action="store_true",
                help="Proceed even if working directory exists (you may lose files!)")
ap.add_argument("--remove", dest="remove", default=False, action="store_true",
                help="Force removal of working directory (you will lose files!)")
ap.add_argument("--graph", dest="graph", default=False, action="store_true",
                help="Create dependency graph as Makeflow.pdf in the workdir")
ap.add_argument("--run", "-r", dest="run", default=False, action="store_true",
                help="Automatically run workflow after generating the manifest")
ap.add_argument("jdl",
                help="AliEn JDL steering the jobs")
ap.add_argument("makeflow_opts", nargs=REMAINDER,
                help="Options for the makeflow command")
args = ap.parse_args()

j2env = jinja2.Environment()
j2env.filters["basename"] = lambda x: os.path.basename(x)
j2env.filters["job_head"] = \
  lambda stage, index=-1: \
    "{stage}{cond_index}.done: run{stage}.sh {stage_input}" \
      .format(stage=stage,
              index=0 if index < 0 else index,
              cond_index=("%04d"%index) if index > -1 else "",
              stage_input=" ".join([ x for x in jdls[stage].get("InputFile") if not re.search("^(/|[^ :]+:)", x) ]))
j2env.filters["job_cmd"] = \
  lambda stage, index=-1, extra_input=[], extra_output=[]: \
    "\t./run{stage}.sh {index} {stage}{cond_index}.done {input_file}" \
      .format(stage=stage,
              index=0 if index < 0 else index,
              cond_index=("%04d"%index) if index > -1 else "",
              input_file=jdls[stage]["InputFilesFromList"][index] if jdls[stage].get("InputFilesFromList", []) else "")

def preprocess_var(s, sh_mode):
  if isinstance(s, list):
    s = ",".join(s)
  if not re.search("#alien_counter(_[^#]+)?#|#alienfilename(/([^/#]+)/([^/#]*)/)?#", s):
    return s
  if sh_mode:
    # Shell mode: rely on script's alienfmt() shell function
    s = "$(alienfmt '%s')" % s
  else:
    # Jinja mode: use Jinja's builtin string formatting
    s = s.replace("#alien_counter#", "#alien_counter_i#")
    s = re.sub("#alien_counter_([^#]+)#", "{{'%\\1'|format(alien_counter)}}", s)
    s = s.replace("#alienfilename#", "{{input_files[alien_counter]|basename()}}")
    s = re.sub("#alienfilename/([^/#]+)/([^/#]*)/#", "{{input_files[alien_counter]|basename()|replace('\\1','\\2')}}", s)
  return s

def gen_runjob(output_file, jdl, dry_run):
  runjob = j2env.from_string("""#!/bin/bash -e
type zip unzip zipinfo timeout readlink
ulimit -c unlimited  # enable core dumps
{% for v in [ "LANG", "LANGUAGE", "LC_ALL", "LC_COLLATE", "LC_CTYPE", "LC_MESSAGES", \
              "LC_MONETARY", "LC_NUMERIC", "LC_TIME", "LC_ALL" ] -%}
export {{v}}=C
{% endfor -%}
JOBID=$1
JOBDIR=$(basename "$0")
JOBDIR=job-${JOBDIR%.*}-$(printf %04d $JOBID)
DONEFILE="$PWD/$2"
INPUTFILE="$3"
shift 3 || shift 2 || true

function alienfmt() {
  local ALIENFMT_STR="$1"
  local INPUTFILE_BASE=${INPUTFILE##*/}

  RE='^(.*)#alien_counter(_([^#]+))?#(.*)$'
  while [[ $ALIENFMT_STR =~ $RE ]]; do
    REPLACE=${BASH_REMATCH[3]}
    [[ $REPLACE ]] || REPLACE=i
    REPLACE=$(printf %${REPLACE} $JOBID)
    ALIENFMT_STR="${BASH_REMATCH[1]}${REPLACE}${BASH_REMATCH[4]}"
  done

  RE='^(.*)#alienfilename(/([^#/]+/[^#/]*)/)?#(.*)$'
  while [[ $ALIENFMT_STR =~ $RE ]]; do
    REPLACE=${BASH_REMATCH[3]}
    [[ $REPLACE ]] && REPLACE='${INPUTFILE_BASE//'$REPLACE'}' || REPLACE='$INPUTFILE_BASE'
    ALIENFMT_STR="${BASH_REMATCH[1]}${REPLACE}${BASH_REMATCH[4]}"
  done

  ALIENFMT_STR=$(eval printf -- \\""$ALIENFMT_STR"\\")
  echo "$ALIENFMT_STR"
}

rm -rf job-$JOBDIR
mkdir job-$JOBDIR
cd job-$JOBDIR
INPUT_LIST=({% for x in input_list -%}
            "{{x}}"
            {% endfor %})
OUTPUT_LIST=({% for x in output_list -%}
             "{{x}}"
             {% endfor %})
{{""}}
# Copy Makeflow-provided files first
for INP in "${INPUT_LIST[@]}"; do
  [[ ${INP%%://*} == $INP ]] || continue  # local file (no protocol)
  [[ $INP != /* ]] || continue            # no full path
  cp -v ../"$INP" .
done
{{""}}
{% if env_cmd -%}
# Set custom environment (handling potential concurrency issues on our side)
LOCKDIR=/tmp/makeflow_env_lock
MAXWAIT=1000
sleep 0.1 &> /dev/null || MAXWAIT=100
for ((I=0; I<$MAXWAIT; I++)); do
  mkdir $LOCKDIR &> /dev/null && break || true
  sleep 0.1 || sleep 1
done
set +e
{{env_cmd}}
ENV_RV=$?
set -e
rmdir $LOCKDIR || true
[[ $ENV_RV == 0 ]] || exit 3
{% else -%}
eval $(/cvmfs/alice.cern.ch/bin/alienv printenv {{packages|join(",")}})
{% endif -%}
{{""}}
# Job environment
{% for x in environment -%}
export {{x}}="{{environment[x]|replace('"', '\\\\"')}}"
{% endfor -%}
{{""}}
# Prepare input box
for INP in "${INPUT_LIST[@]}"; do
  PROTO=${INP%%://*}
  UNPACK=
  [[ $INP == /* ]] || continue  # Makeflow files have been copied already
  [[ $PROTO != $INP ]] || { PROTO=local; [[ $INP == /* ]] || INP="../$INP"; }
  [[ $PROTO == local || $PROTO == root ]] || { echo "Input protocol $PROTO not supported"; exit 1; }
  [[ $INP == *,unpack ]] && { UNPACK=1; INP=${INP//,unpack}; }
  for ((I=1; I<=5; I++)); do
    ERR=0
    echo "Transferring $INP to the input box (attempt $I/5)"
    case $PROTO in
      local) cp -v "$INP" . && break || ERR=$? ;;
      root)  timeout -s 9 ${ALIEN_JDL_COPYTIMEOUT:-100} xrdcp -f "$INP" . && break || ERR=$? ;;
    esac
  done
  [[ $ERR == 0 ]] || { echo "Error copying files into the input box"; exit 1; }
  if [[ $UNPACK ]]; then
    INP=${INP##*/}
    echo "Unpacking then removing $INP"
    case "$INP" in
      *.tar.bz2) UNPACK_CMD="tar xjvvf" ;;
      *.tar.gz)  UNPACK_CMD="tar xzvvf" ;;
      *.zip)     UNPACK_CMD="unzip"     ;;
      *) echo "Unsupported archive format: $INP"; exit 1; ;;
    esac
    $UNPACK_CMD "$INP" || ERR=$?
    rm -f "$INP"
  fi
  [[ $ERR == 0 ]] || { echo "Error unpacking files into the input box"; exit 1; }
done
echo "List of files in the working directory (recursive)"
find . -ls
echo "List of files ends here"
echo "Output will be in $ALIEN_JDL_OUTPUTDIR"

# Execute command and ignore errors
ARGS="{{args}}"
PROG="{{executable}}"
{% if dry_run -%}
echo "Doing nothing: dry run" >> stdout.log
MAINERR=0
{% else -%}
type "$PROG" &> /dev/null || PROG="{{executable|basename()}}"
type "$PROG" &> /dev/null || PROG="$(find "$ALIDPG_ROOT" -name "{{executable|basename()}}" -print -quit || true)"
[[ "$PROG" ]] || PROG="./{{executable|basename()}}"
[[ -x "$PROG" ]] || PROG="bash $PROG"
MAINERR=0
{ env; echo + $PROG $ARGS; } >> stdout.log
$PROG $ARGS $*  > >(tee -a stdout.log{% if no_live_out %} &> /dev/null{% endif %}) \\
               2> >(tee -a stderr.log{% if no_live_out %} &> /dev/null{% endif %}) || MAINERR=$?
{% endif -%}
[[ $MAINERR != 0 ]] && echo "Exited with errors ($MAINERR)" || echo "Exited with no errors";
{ echo + $PROG $ARGS "exited with $MAINERR"; } >> stdout.log

# Produce validation report
touch validation_report.txt
grep -H -n -E 'std::bad_alloc|Segmentation violation|Segmentation fault|Bus error|floating point exception|Killed|busy flag cleared|Cannot Build the PAR Archive|\\*\\*\\* glibc detected \\*\\*\\*|E-AliCDBGrid::PutEntry:|F-AliCDBGrid::|E-TAlienFile::ReadBuffer: The remote \\(removed\\) file is not open' *.log > validation_report.txt 2> /dev/null || true
for CORE in core*; do
  [[ -e $CORE ]] || continue
  echo "$CORE:0:core dumped"
done >> validation_report.txt 2> /dev/null
{{""}}
{% if dry_run -%}
# Dry run: creating dummy output files to make Makeflow happy
pushd $(dirname $DONEFILE) &> /dev/null
  [[ $ALIEN_JDL_TRANSFERTOMAKEFLOW ]] && touch $ALIEN_JDL_TRANSFERTOMAKEFLOW
popd &> /dev/null
{% else -%}
# Make some files available to Makeflow directly too
[[ $ALIEN_JDL_TRANSFERTOMAKEFLOW ]] && cp -v $ALIEN_JDL_TRANSFERTOMAKEFLOW $(dirname $DONEFILE)/
{% endif -%}
{{""}}
# Compress output according to the output list
mkdir to_transfer
shopt -s extglob
for OUT in "${OUTPUT_LIST[@]}"; do
  ZIP=${OUT%%:*}
  FILES=${OUT#*:}
  [[ $ZIP == $OUT ]] && { echo "Not archiving $OUT"; mv -v $OUT to_transfer/ || true; continue; }
  [[ $FILES =~ \.root(,|$) ]] && ZIP_COMP="-0" || ZIP_COMP="-9"
  FILES=${FILES//,/ }
  echo $ZIP will contain $FILES
  ZIPERR=0
  zip $ZIP_COMP tmparchive.zip $FILES || ZIPERR=$?  # exitcode 12 is fine ==> "nothing to do"
  [[ $ZIPERR == 12 ]] && { echo "Zip $ZIP would be empty: not creating"; continue; } \\
                      || [[ $ZIPERR == 0 ]]
  rm -f $FILES  # same files cannot be in more than one archive
  mv tmparchive.zip to_transfer/$ZIP
done

# Copy files to destination (filesystem or xrootd)
PROTO=${ALIEN_JDL_OUTPUTDIR%%://*}
[[ $PROTO != $ALIEN_JDL_OUTPUTDIR ]] || { PROTO=local; mkdir -p $ALIEN_JDL_OUTPUTDIR; }
[[ $PROTO == local || $PROTO == root ]] || { echo "Output protocol $PROTO not supported;" exit 1; }
: ${ALIEN_JDL_COPYNUMJOBS:=15}
COPYSTATEDIR=$PWD/copystate
mkdir $COPYSTATEDIR
pushd to_transfer
  COPYCOUNT=0
  while read FILE; do
    ln -nfs $FILE $COPYSTATEDIR/$COPYCOUNT
    COPYCOUNT=$((COPYCOUNT+1))
  done < <(find . -type f | sed -e 's|^\./||')
  for ((I=0; I<ALIEN_JDL_COPYNUMJOBS; I++)); do
    ( while [[ 1 ]]; do
        PLACEHOLDER=$(find $COPYSTATEDIR -type l -print -quit 2> /dev/null)
        [[ $PLACEHOLDER ]] || break
        FILE=$(readlink $PLACEHOLDER)
        rm $PLACEHOLDER 2> /dev/null || continue
        for ((J=1; J<=5; J++)); do
          ERR=0
          echo "Transferring $FILE to $ALIEN_JDL_OUTPUTDIR (attempt $J/5)"
          case $PROTO in
            local) mkdir -p $(dirname "$ALIEN_JDL_OUTPUTDIR/$FILE"); cp -v $FILE $ALIEN_JDL_OUTPUTDIR/$FILE && break || ERR=$? ;;
            root)  timeout -s 9 ${ALIEN_JDL_COPYTIMEOUT:-100} xrdcp -f $FILE $ALIEN_JDL_OUTPUTDIR/$FILE && break || ERR=$? ;;
          esac
        done
        [[ $ERR == 0 ]] || { echo "Error copying output files"; exit 1; }
      done
    ) &
  done
  wait
popd

# Cleanup all
rm -rf *

# Signal success
echo Workflow execution completed: $PROG $ARGS exited with $MAINERR
touch $DONEFILE
""")
  with open(output_file, "w") as mf:
    mf.write(runjob.render(output_list = jdl["Output"],
                           input_list  = jdl["InputFile"],
                           packages    = jdl["Packages"],
                           env_cmd     = jdl.get("EnvironmentCommand", None),
                           no_live_out = jdl["NoLiveOutput"],
                           environment = jdl["Environment"],
                           dry_run     = dry_run,
                           executable  = jdl["Executable"],
                           args        = jdl["SplitArguments"]))
  os.chmod(output_file, int("755", 8))

def get_alien_xml(pattern, joba, jobb, input_files):
  axml = j2env.from_string(
"""<alien>
  <collection name="alien_collection.xml">
    {%- for alien_counter in range(joba,jobb+1) %}
    <event name="{{alien_counter}}">
      <file turl="***TURL***" type="f"/>
    </event>{% endfor %}
  </collection>
</alien>
""".replace("***TURL***", preprocess_var(pattern, sh_mode=False)))
  return axml.render(joba=joba, jobb=jobb, input_files=input_files)

def get_preprocessed_jdl(jdl_fn, override={}, append={}, delete=[]):
  m = re.search("^(.*)(\.[^.]+)$", jdl_fn)
  jdl_fn_override = "%s_override%s" % (m.group(1),m.group(2)) if m else jdl_fn + "_override"
  with open(jdl_fn) as fp:
    jdl_buf = fp.read()
  if os.path.isfile(jdl_fn_override):
    with open(jdl_fn_override) as fp:
      jdl_buf += "\n" + fp.read()
  jdl = classad.parse(jdl_buf, ignore_errors=True)

  # Process overrides
  for k in override:
    jdl[k] = override[k]

  # Consider Arguments and SplitArguments equivalent
  if "Arguments" in jdl:
    jdl["SplitArguments"] = jdl["Arguments"]

  # Command-line arguments, supporting AliEn variables
  jdl["SplitArguments"] = preprocess_var(jdl["SplitArguments"], sh_mode=True)

  # Packages (filter out jemalloc)
  jdl["Packages"] = [ x for x in jdl["Packages"] if not "jemalloc" in x ]

  # Read input files list (optional)
  try:
    jdl["InputFilesFromList"] = [ x.strip() for x in
                                  open(os.path.basename(jdl.get("InputDataCollection", ""))).readlines()
                                  if not re.search("(^\s*#|^\s*$)", x) ]
  except:
    jdl["InputFilesFromList"] = []

  try:
    trunc = int(jdl.get("LimitInputFiles"))
  except Exception:
    trunc = 0
  if trunc > 0:
    jdl["InputFilesFromList"] = jdl["InputFilesFromList"][:trunc]
    jdl["LimitInputFiles"] = trunc
  else:
    jdl["LimitInputFiles"] = len(jdl["InputFilesFromList"])

  # Job range ("Split" parameter)
  m = re.search("^([^:]+)(:([0-9]+)-([0-9]+))?$", jdl["Split"])
  if m.group(3) and m.group(4):
    jdl["JobRange"] = [ int(m.group(3)), int(m.group(4)) ]  # production
  else:
    jdl["JobRange"] = [ 0, max(0, len(jdl["InputFilesFromList"])-1) ]  # file

  # Remove @disk spec from output list
  jdl["Output"] = [ o.split("@", 1)[0] for o in jdl["Output"] ]

  # Input list: base path only (assume files are in the cwd or a "system" one), exclude OCDB
  jdl["InputFile"] = [ os.path.basename(x) if x.startswith("LF:") else x
                       for x in jdl.get("InputFile", []) ]
  jdl["InputFile"] = [ x for x in jdl["InputFile"] if not x.startswith("OCDB") ]

  # Process appends
  for k in append:
    if k in jdl:
      jdl[k] += append[k]

  # Determine whether this JDL comes from a MC or a Reco
  if jdl.get("Executable", "").endswith("aliroot_dpgsim.sh"):
    jdl["JobType"] = "MC"
  elif "/aliroot_dpg" in jdl.get("Executable", ""):
    jdl["JobType"] = "Reco"
  else:
    jdl["JobType"] = None

  # Job environment
  environment = {}
  for v in jdl.get("JDLVariables", []):
    environment["ALIEN_JDL_"+v.upper()] = preprocess_var(jdl.get(v, ""), sh_mode=True)  # with ALIEN_JDL_
  for v in jdl.get("ExtraVariables", []):
    environment[v] = preprocess_var(jdl.get(v, ""), sh_mode=True)  # exported as-is
  jdl["Environment"] = environment

  # Output requested not in zip files
  if jdl.get("DontArchive", None) == "1":
    jdl["DontArchive"] = True
    new_output = []
    for o in jdl["Output"]:
      if ":" in o:
        new_output += o.split(":", 1)[1].split(",")
      else:
        new_output.append(o)
    jdl["Output"] = new_output
  else:
    jdl["DontArchive"] = False

  # Save everything but no input files
  if jdl.get("SaveAll", None) == "1":
    jdl["SaveAll"] = True
    jdl["Output"] = [ "!(%s)" % "|".join(jdl["InputFile"]) ]
  else:
    jdl["SaveAll"] = False

  # No live output (convert to boolean)
  jdl["NoLiveOutput"] = jdl.get("NoLiveOutput", None) == "1"

  # Remove unneeded variables (cleanup)
  all_vars = jdl.keys()
  whitelist = [ "SplitArguments", "Executable", "Packages", "JobRange", "JobRange", "Output",
                "InputFile", "Environment", "NextStages", "OutputDir", "EnvironmentCommand",
                "SaveAll", "DontArchive", "NoLiveOutput", "InputFilesFromList", "JobType" ] + \
              list(override.keys()) + list(append.keys())
  for k in list(all_vars):
    if k in delete or k not in whitelist:
      del jdl[k]

  return jdl

# Store all input files for all jobs (dictionary of arrays): will be used by Jinja
all_inputs = {}
stages = None

# First tier of jobs: it also determines the job type (exit if unknown)
jdl = get_preprocessed_jdl(args.jdl)
if jdl["JobType"] == "MC":
  all_inputs["sim"] = jdl["InputFile"]
  stages = stages_mc
  steps = [ "MCPass" ]
elif jdl["JobType"] == "Reco":
  all_inputs["cpass0"] = jdl["InputFile"]
  stages = stages_reco
  steps = [ "CPass0", "CPass0_SPC", "CPass1", "PPass" ]
else:
  print("Cannot recognize job type, only MC and Reco are supported")
  exit(1)

# All JDLs are here. Dictionary whose labels are the stages defined above
jdls = OrderedDict()

# XMLs to generate (dict whose names are filenames)
xmls = {}

# Text lists to generate (dict whose names are filenames)
txts = {}

# Create all JDLs for MC
if jdl["JobType"] == "MC":

  # sim
  jdl["Output"] = list(set(jdl.get("Output", []) + [ "core*", "validation_report.txt" ]))
  jdls["sim"] = jdl

  # simResMerge
  jdls["simResMerge"] = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "spcm_archive.zip:pyxsec*.root,AODQA.root,AliAOD*.root,FilterEvents_Trees*.root,*.stat*,EventStat_temp*.root,Residual*.root,TOFcalibTree.root,std*,fileinfo*.log@disk=2",
                             "validation_report.txt",
                             "core*" ],
                 "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)),
                                           "SpacePointCalibrationMerge", "001"),
                 "Executable": "merge.sh",
                 "InputXML": "residualtrees.xml",
                 "LPMCollectionEntity": "FilterEvents_Trees.root",
                 "NoGridConnect": "1",
                 "SplitArguments": "" },
      append={ "InputFile": [ "residualtrees.xml" ],
               "JDLVariables": [ "LPMCollectionEntity", "InputXML", "NoGridConnect" ] },
      delete=[ "JobRange", "InputFilesFromList" ])
  xmls["residualtrees.xml"] = \
    get_alien_xml(os.path.join(jdl["OutputDir"], "FilterEvents_Trees.root"),
                  jdl["JobRange"][0], jdl["JobRange"][1], jdl["InputFilesFromList"])

  # simQaMerge
  inputfile = "finalqa.txt" if jdl["DontArchive"] or jdl["SaveAll"] else "finalqa.xml"
  jdls["simQaMerge"] = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "QA_merge_log_archive.zip:std*,fileinfo*.log@disk=1",
                             "QA_merge_archive.zip:*QAresults*.root,EventStat_temp*.root,trending*.root,event_stat*.root,*.stat*@disk=3",
                             "validation_report.txt",
                             "core*" ],
                 "OutputDir": os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)),
                 "Executable": "aliprod_train_merge.sh",
                 "SplitArguments": "%s 5" % inputfile},  # 5 means "final stage"
      append={ "InputFile": [ inputfile, "aliprod_train_merge.sh" ] },
      delete=[ "JobRange" ])
  if inputfile.endswith(".xml"):
    xmls[inputfile] = \
      get_alien_xml(os.path.join(jdl["OutputDir"], "QA_archive.zip"),
                                 jdl["JobRange"][0], jdl["JobRange"][1],
                                 jdl["InputFilesFromList"])
  elif inputfile.endswith(".txt"):
    pattern = preprocess_var(jdl["OutputDir"], sh_mode=False)
    txts[inputfile] = \
      "\n".join([ j2env.from_string(pattern).render(alien_counter=ji, input_files=jdl["InputFilesFromList"])
                  for ji in range(jdl["JobRange"][0], jdl["JobRange"][1]+1) ]) + "\n"
    del pattern
  else:
    assert False, "inputfile should either end with .xml or .txt"
  del inputfile

  # simQaPlots
  inputs = [ "QAresults.root", "QAresults_merged.root", "QAresults_barrel.root",
             "QAresults_outer.root", "FilterEvents_Trees.root", "event_stat.root",
             "event_stat_barrel.root", "event_stat_outer.root" ] \
           if jdl["DontArchive"] or jdl["SaveAll"] else \
           [ "QA_merge_archive.zip" ]
  jdls["simQaPlots"] = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "qa_plots/*", "std*", "core*", "validation_report.txt" ],
                 "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "QAplots_passMC"),
                 "Executable": "qa_plots.sh",
                 "SplitArguments": " ".join([ os.path.join(preprocess_var(os.path.dirname(jdl["OutputDir"]), sh_mode=False), x) for x in inputs ]) },
      append={ "InputFile": [ "qa_plots.sh" ] },
      delete=[ "NextStages", "JobRange", "InputFilesFromList" ])
  del inputs

elif jdl["JobType"] == "Reco":

  # Run number in integer and string formats
  runNumberStr = os.path.basename(jdl["InputFilesFromList"][0])[2:11]
  runNumber = int(runNumberStr)

  #
  # CPass0
  #

  # cpass0
  jdl["Output"] = list(set(jdl.get("Output", []) + [ "core*", "validation_report.txt" ]))
  jdls["cpass0"] = jdl

  # cpass0Merge
  jdls["cpass0Merge"] = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "log_archive.zip:*.log@disk=2",
                             "ocdb_archive.zip:CalibObjects.root,meanITSVertex.root,fitITSVertex.root,cpassStat.root@disk=4",
                             "OCDB.tar.bz2",
                             "validation_report.txt",
                             "core*" ],
                 "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "OCDB"),
                 "Executable": "aliroot_dpgMergeMakeOCDB_CPass0.sh",
                 "Arguments": "calibobjects.xml %d local://./OCDB fileAccessMethod=copyXMLcollection makeOCDB=1 calibObjectsFileName=CalibObjects.root" % runNumber,
                 "CreateOCDBArchive": "OCDB.tar.bz2"
               },
      append={ "InputFile"   : [ "calibobjects.xml" ],
               "JDLVariables": [ "CreateOCDBArchive" ] },
      delete=[ "JobRange", "InputFilesFromList" ])
  xmls["calibobjects.xml"] = \
    get_alien_xml(os.path.join(jdl["OutputDir"], "CalibObjects.root"),
                               jdl["JobRange"][0], jdl["JobRange"][1], jdl["InputFilesFromList"])

  # cpass0ResMerge
  jdls["cpass0ResMerge"] = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "spcm_archive.zip:pyxsec*.root,AODQA.root,AliAOD*.root,FilterEvents_Trees*.root,*.stat*,EventStat_temp*.root,Residual*.root,TOFcalibTree.root,std*,fileinfo*.log@disk=2",
                             "validation_report.txt",
                             "core*" ],
                 "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)),
                                           "ResidualMerge", "001"),
                 "Executable": "merge.sh",
                 "InputXML": "residualtrees.xml",
                 "LPMCollectionEntity": "ResidualTrees.root",
                 "NoGridConnect": "1",
                 "SplitArguments": "" },
      append={ "InputFile": [ "residualtrees.xml" ],
               "JDLVariables": [ "LPMCollectionEntity", "InputXML", "NoGridConnect" ] },
      delete=[ "JobRange", "InputFilesFromList" ])
  xmls["residualtrees.xml"] = \
    get_alien_xml(os.path.join(jdl["OutputDir"], "ResidualTrees.root"),
                               jdl["JobRange"][0], jdl["JobRange"][1], jdl["InputFilesFromList"])

  # cpass0SpBins
  jdls["cpass0SpBins"] = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "log_archive.zip:*.log,*.list@disk=2",
                             "root_archive.zip:*.root@disk=2",
                             "validation_report.txt",
                             "core*" ],
                 "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "ResidualMerge", "TPCSPCalibration"),
                 "Executable": "aliroot_procVDTime.sh",
                 "Arguments": "tpcSPCalibration.xml %s" % runNumberStr,
                 "targetOCDBDir": "local://./OCDB",  # output for OCDB files, it's "same" (raw://) on the Grid (OCDB is prepopulated from tarball)
                 "useTOFBC": "false",
                 "transferToMakeflow": "timeBins.log"
               },
      append={ "InputFile": [ "tpcSPCalibration.xml",
                              os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "OCDB", "OCDB.tar.bz2,unpack") ],
               "JDLVariables": [ "targetOCDBDir", "useTOFBC", "transferToMakeflow" ] },
      delete=[ "JobRange", "InputFilesFromList" ])
  xmls["tpcSPCalibration.xml"] = \
    get_alien_xml(
      os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)),
                   "ResidualMerge", "001", "ResidualTrees.root"), 0, 0, [])

  # cpass0SpDistMaps
  jdls["cpass0SpDistMaps"] = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "log_archive.zip:*.log,*.list@disk=2",
                             "root_archive.zip:alitpcdcalibres.root,voxelResTree.root@disk=2",
                             "validation_report.txt",
                             "core*" ],
                 "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "ResidualMerge", "TPCSPCalibration", "#alienfilename#"),
                 "Executable": "aliroot_procTBinMap.sh",
                 "Arguments": ""  # will append arguments through the Makeflow manifest
               },
      append={ "InputFile": [ os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "ResidualMerge", "TPCSPCalibration", "alitpcdcalibres.root"),
                              os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "OCDB", "OCDB.tar.bz2,unpack") ] },
      delete=[ "JobRange", "InputFilesFromList" ])

  # cpass0SpCalib
  jdls["cpass0SpCalib"] = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "log_archive.zip:*.log,*.list@disk=2",
                             "validation_report.txt",
                             "OCDB_SPC.tar.bz2",
                             "core*" ],
                 "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "ResidualMerge", "TPCSPCalibration", "CorrectionMaps"),
                 "Executable": "aliroot_createOCDBTPCSPDistCalib.sh",
                 "Arguments": "inputFileList=alitpcdcalibres.txt startRun=%d endRun=%d" % (runNumber, runNumber),
                 "targetOCDBDir": "local://./OCDB",
                 "CreateOCDBArchive": "OCDB_SPC.tar.bz2"
               },
      append={ "InputFile": [ "alitpcdcalibres.txt,late",  # late creation
                              os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)), "OCDB", "OCDB.tar.bz2,unpack") ],
               "JDLVariables": [ "targetOCDBDir", "CreateOCDBArchive" ] },
      delete=[ "JobRange", "InputFilesFromList" ])

  #
  # CPass1
  #

  # cpass1
  jdls["cpass1"] = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "log_archive.zip:qa*.log,calib.log,filtering.log,mergeQA_outer.log,mergeQA_barrel.log,rec.log,stderr.log,stdout.log@disk=1",
                             "root_archive.zip:AliESDs_Barrel.root,CalibObjects.root,*.ESD.tag.root,TOFcalibTree.root,T0AnalysisTree.root@disk=2",
                             "QA_archive.zip:QAresults_barrel.root,QAresults_outer.root,*.stat.qa*@disk=2",
                             "EventStat_temp*.root@disk=2",
                             "ResidualTrees.root@disk=2",
                             "FilterEvents_Trees.root@disk=2",
                             "validation_report.txt",
                             "core*" ],
                 "OutputDir": jdl["OutputDir"].replace("/cpass0_pass1/", "/cpass1_pass1/"),
                 "Executable": "aliroot_dpgCPass1.sh",
                 "LPMPass": "1",
                 "LPMPassName": "cpass1_pass1",
                 "LPMRAWPassID": "1",
                 "LPMCPassMode": "1"
               },
      append={ "InputFile": [ os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)),
                                           "ResidualMerge", "TPCSPCalibration", "CorrectionMaps", "OCDB_SPC.tar.bz2,unpack") ] },
      delete=[])

  # cpass1Merge
  jdls["cpass1Merge"] = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "log_archive.zip:*.log@disk=2",
                             "ocdb_archive.zip:CalibObjects.root,meanITSVertex.root,fitITSVertex.root,cpassStat.root@disk=4",
                             "OCDB.tar.bz2",
                             "validation_report.txt",
                             "core*" ],
                 "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdls["cpass1"]["OutputDir"], sh_mode=False)), "OCDB"),
                 "Executable": "aliroot_dpgMergeMakeOCDB_CPass1.sh",
                 "Arguments": "calibobjects_cpass1.xml %d local://./OCDB fileAccessMethod=copyXMLcollection makeOCDB=1 calibObjectsFileName=CalibObjects.root" % runNumber,
                 "CreateOCDBArchive": "OCDB.tar.bz2"
               },
      append={ "InputFile": [ os.path.join(os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)),
                                           "ResidualMerge", "TPCSPCalibration", "CorrectionMaps", "OCDB_SPC.tar.bz2,unpack"),
                              "calibobjects_cpass1.xml" ],
               "JDLVariables": [ "CreateOCDBArchive" ] },
      delete=[ "JobRange", "InputFilesFromList" ])
  xmls["calibobjects_cpass1.xml"] = \
    get_alien_xml(os.path.join(jdls["cpass1"]["OutputDir"], "CalibObjects.root"),
                  jdls["cpass1"]["JobRange"][0], jdls["cpass1"]["JobRange"][1],
                  jdls["cpass1"]["InputFilesFromList"])

  # cpass1ResMerge
  jdls["cpass1ResMerge"] = \
    get_preprocessed_jdl(args.jdl,
              override={ "Output": [ "spcm_archive.zip:pyxsec*.root,AODQA.root,AliAOD*.root,FilterEvents_Trees*.root,*.stat*,EventStat_temp*.root,Residual*.root,TOFcalibTree.root,std*,fileinfo*.log@disk=2",
                                     "validation_report.txt",
                                     "core*" ],
                         "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdls["cpass1"]["OutputDir"], sh_mode=False)),
                                                   "MergedTrees", "001"),
                         "Executable": "merge.sh",
                         "InputXML": "eventtrees_cpass1.xml",
                         "LPMCollectionEntity": "FilterEvents_Trees.root,TOFcalibTree.root",
                         "NoGridConnect": "1",
                         "SplitArguments": "" },
              append={ "InputFile": [ "eventtrees_cpass1.xml" ],
                       "JDLVariables": [ "LPMCollectionEntity", "InputXML", "NoGridConnect" ] },
              delete=[ "JobRange", "InputFilesFromList" ])
  xmls["eventtrees_cpass1.xml"] = \
    get_alien_xml(os.path.join(jdls["cpass1"]["OutputDir"], "ResidualTrees.root"),
                               jdls["cpass1"]["JobRange"][0], jdls["cpass1"]["JobRange"][1],
                               jdls["cpass1"]["InputFilesFromList"])

  # cpass1QaMerge
  inputfile = "finalqa.txt" if jdls["cpass1"]["DontArchive"] or jdls["cpass1"]["SaveAll"] else "finalqa.xml"
  jdls["cpass1QaMerge"] = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "QA_merge_log_archive.zip:std*,fileinfo*.log@disk=1",
                             "QA_merge_archive.zip:*QAresults*.root,EventStat_temp*.root,trending*.root,event_stat*.root,*.stat*@disk=3",
                             "validation_report.txt",
                             "core*" ],
                 "OutputDir": os.path.dirname(preprocess_var(jdls["cpass1"]["OutputDir"], sh_mode=False)),
                 "Executable": "train_merge.sh",
                 "SplitArguments": "%s 5" % inputfile },  # 5 means "final stage"
      append={ "InputFile": [ inputfile ] },
      delete=[ "JobRange" ])
  if inputfile.endswith(".xml"):
    xmls[inputfile] = \
      get_alien_xml(os.path.join(jdls["cpass1"]["OutputDir"], "QA_archive.zip"),
                                 jdls["cpass1"]["JobRange"][0], jdls["cpass1"]["JobRange"][1],
                                 jdls["cpass1"]["InputFilesFromList"])
  else:
    pattern = preprocess_var(jdls["cpass1"]["OutputDir"], sh_mode=False)
    txts[inputfile] = "\n".join([ j2env.from_string(pattern).render(alien_counter=ji, input_files=jdls["cpass1"]["InputFilesFromList"])
                                  for ji in range(jdls["cpass1"]["JobRange"][0], jdls["cpass1"]["JobRange"][1]+1) ]) + "\n"
    del pattern
  del inputfile

  # cpass1QaPlots
  inputs = [ "QAresults.root", "QAresults_merged.root", "QAresults_barrel.root",
             "QAresults_outer.root", "FilterEvents_Trees.root", "event_stat.root",
             "event_stat_barrel.root", "event_stat_outer.root" ] \
           if jdls["cpass1"]["DontArchive"] or jdls["cpass1"]["SaveAll"] else \
           [ "QA_merge_archive.zip" ]
  jdls["cpass1QaPlots"] = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "qa_plots/*", "std*", "core*", "validation_report.txt" ],
                 "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdls["cpass1"]["OutputDir"], sh_mode=False)), "QAplots_CPass1"),
                 "Executable": "qa_plots.sh",
                 "SplitArguments": " ".join([ os.path.join(os.path.dirname(preprocess_var(jdls["cpass1"]["OutputDir"], sh_mode=False)), x) for x in inputs ]) },
      append={ "InputFile": [ "qa_plots.sh" ] },
      delete=[ "JobRange", "InputFilesFromList" ])
  del inputs

  #
  # PPass
  #

  # ppass
  jdls["ppass"] = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "log_archive.zip:*.log@disk=1",
                             "root_archive.zip:AliESDs.root,AliESDfriends.root,Merged.QA.Data*.root,*.ESD.tag.root@disk=2",
                             "QA_archive.zip:QAresults*.root,*.stat.qa*,event_stat*.root,trending*.root@disk=2",
                             "aod_archive.zip:pyxsec*.root,AODQA.root,AliAOD*.root,FilterEvents_Trees*.root,*.stat.aod@disk=2",
                             "EventStat_temp*.root@disk=2",
                             "validation_report.txt",
                             "core*" ],
                 "OutputDir": jdl["OutputDir"].replace("/cpass0_pass1/", "/pass1/"),
                 "Executable": "aliroot_dpgPPass.sh",
                 "SplitArguments": os.path.join(os.path.dirname(jdl["InputFilesFromList"][0]), "#alienfilename#"),
                 "LPMPass": "2",
                 "LPMPassName": "pass1",
                 "LPMRAWPassID": "2",
                 "LPMCPassMode": "-1",
                 "RAWPRODTYPE": "ppass"
               },
      append={ "InputFile": [ os.path.join(os.path.dirname(preprocess_var(jdls["cpass1"]["OutputDir"], sh_mode=False)), "OCDB", "OCDB.tar.bz2,unpack") ],
               "JDLVariables": [ "RAWPRODTYPE" ] },
      delete=[])

  # ppassResMerge
  jdls["ppassResMerge"] = \
    get_preprocessed_jdl(args.jdl,
              override={ "Output": [ "spcm_archive.zip:pyxsec*.root,AODQA.root,AliAOD*.root,FilterEvents_Trees*.root,*.stat*,EventStat_temp*.root,Residual*.root,TOFcalibTree.root,std*,fileinfo*.log@disk=2",
                                     "validation_report.txt",
                                     "core*" ],
                         "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdls["ppass"]["OutputDir"], sh_mode=False)),
                                                   "MergedTrees", "001"),
                         "Executable": "merge.sh",
                         "InputXML": "eventtrees_ppass.xml",
                         "LPMCollectionEntity": "FilterEvents_Trees.root",
                         "NoGridConnect": "1",
                         "SplitArguments": "" },
              append={ "InputFile": [ "eventtrees_ppass.xml" ],
                       "JDLVariables": [ "LPMCollectionEntity", "InputXML", "NoGridConnect" ] },
              delete=[ "JobRange", "InputFilesFromList" ])
  xmls["eventtrees_ppass.xml"] = \
    get_alien_xml(os.path.join(jdls["ppass"]["OutputDir"], "ResidualTrees.root"),
                               jdls["ppass"]["JobRange"][0], jdls["ppass"]["JobRange"][1],
                               jdls["ppass"]["InputFilesFromList"])

  # ppassQaMerge
  inputfile = "finalqa_ppass.txt" if jdls["ppass"]["DontArchive"] or jdls["ppass"]["SaveAll"] else "finalqa_ppass.xml"
  jdls["ppassQaMerge"] = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "QA_merge_log_archive.zip:std*,fileinfo*.log@disk=1",
                             "QA_merge_archive.zip:*QAresults*.root,EventStat_temp*.root,trending*.root,event_stat*.root,*.stat*@disk=3",
                             "validation_report.txt",
                             "core*" ],
                 "OutputDir": os.path.dirname(preprocess_var(jdls["ppass"]["OutputDir"], sh_mode=False)),
                 "Executable": "train_merge.sh",
                 "SplitArguments": "%s 5" % inputfile },  # 5 means "final stage"
      append={ "InputFile": [ inputfile ] },
      delete=[ "JobRange" ])
  if inputfile.endswith(".xml"):
    xmls[inputfile] = \
      get_alien_xml(os.path.join(jdls["ppass"]["OutputDir"], "QA_archive.zip"),
                                 jdls["ppass"]["JobRange"][0], jdls["ppass"]["JobRange"][1],
                                 jdls["ppass"]["InputFilesFromList"])
  else:
    pattern = preprocess_var(jdls["ppass"]["OutputDir"], sh_mode=False)
    txts[inputfile] = "\n".join([ j2env.from_string(pattern).render(alien_counter=ji, input_files=jdls["ppass"]["InputFilesFromList"])
                                  for ji in range(jdls["ppass"]["JobRange"][0], jdls["ppass"]["JobRange"][1]+1) ]) + "\n"
    del pattern
  del inputfile

  # ppassQaPlots
  inputs = [ "QAresults.root", "QAresults_AOD.root", "QAresults_merged.root", "QAresults_barrel.root",
             "QAresults_outer.root", "FilterEvents_Trees.root", "event_stat.root",
             "event_stat_barrel.root", "event_stat_outer.root" ] \
           if jdls["ppass"]["DontArchive"] or jdls["ppass"]["SaveAll"] else \
           [ "QA_merge_archive.zip" ]
  jdls["ppassQaPlots"] = \
    get_preprocessed_jdl(args.jdl,
      override={ "Output": [ "qa_plots/*", "std*", "core*", "validation_report.txt" ],
                 "OutputDir": os.path.join(os.path.dirname(preprocess_var(jdls["ppass"]["OutputDir"], sh_mode=False)), "QAplots_PPass"),
                 "Executable": "qa_plots.sh",
                 "SplitArguments": " ".join([ os.path.join(os.path.dirname(preprocess_var(jdls["ppass"]["OutputDir"], sh_mode=False)), x) for x in inputs ]) },
      append={ "InputFile": [ "qa_plots.sh" ] },
      delete=[ "JobRange", "InputFilesFromList" ])
  del inputs

else:
  assert False, "should not happen: jdl[\"JobType\"] should be handled at this point"

# Merge validation reports (common to all)
jdls["mergeValidationReports" + jdl["JobType"]] = \
  get_preprocessed_jdl(args.jdl,
    override={ "Output": [ "validation_report_full.txt" ],
               "OutputDir": os.path.dirname(preprocess_var(jdl["OutputDir"], sh_mode=False)),
               "Executable": "merge_validation_reports.sh",
               "Arguments": "validation_report_list.txt",
               "transferToMakeflow": "validation_report_full.txt"
             },
    append={ "InputFile": [ "merge_validation_reports.sh", "validation_report_list.txt,late" ],
             "JDLVariables": [ "transferToMakeflow" ] },
    delete=[ "JobRange", "InputFilesFromList" ])
if jdl["JobType"] == "Reco":
  jdls["mergeValidationReports" + jdl["JobType"]]["OutputDir"] = os.path.dirname(jdls["mergeValidationReports" + jdl["JobType"]]["OutputDir"])

if args.parse or args.parse_only:
  for j in jdls:
    print("# JSONized JDL: %s" % j)
    print(json.dumps(jdls[j], indent=2))
    print("# End of %s" % j)
    print()
  if args.parse_only:
    exit(0)

# Summary
print("""Running the workflow with the following configuration:

Packages:
%(packages)s

%(numjobs)d total jobs, with job IDs from %(joba)d to %(jobb)d (included), will execute the command:
%(command)s

Required files (must be in the current directory, will be made available to each job):
%(reqd)s

Input files:
%(input)s

Output files (archives with content listed):
%(output)s

Output directory:
%(outdir)s

Environment variables available to the jobs:
%(env)s
""" % { "packages" : " * "+"\n * ".join(jdl["Packages"]) if not "EnvironmentCommand" in jdl else \
                     " * Custom environment command: " + jdl["EnvironmentCommand"],
        "numjobs"  : jdl["JobRange"][1]-jdl["JobRange"][0]+1,
        "joba"     : jdl["JobRange"][0],
        "jobb"     : jdl["JobRange"][1],
        "command"  : jdl["Executable"] + " " + jdl["SplitArguments"],
        "reqd"     : " * "+"\n * ".join(jdl["InputFile"]),
        "input"    : " * "+"\n * ".join(jdl["InputFilesFromList"]),
        "output"   : " * "+"\n * ".join([ " ==> ".join(x.split(":", 1)) for x in jdl["Output"] ]),
        "outdir"   : jdl["OutputDir"],
        "env"      : " * "+"\n * ".join([ "%s ==> %s"%(x,jdl["Environment"][x]) for x in jdl["Environment"]])

})
sys.stdout.flush()
if args.summary:
  exit(0)

# Create working directory. From now on we write to disk, not before
if args.remove:
  try:
    rmtree(args.work_dir)
  except:
    pass
try:
  os.mkdir(args.work_dir)
except OSError as e:
  if e.errno == errno.EEXIST:
    if not args.force:
      print("Cannot create output directory \"%s\": remove existing one first" % args.work_dir)
      exit(1)
    else:
      print("WARNING: output directory \"%s\" exists already" % args.work_dir)
  else:
    print("Cannot create output directory \"%s\": %s" % (args.workdir, e))
    exit(1)

# Remove all done files found
if args.force:
  print("Removing all existing state files")
  for f in glob(os.path.join(args.work_dir, "*.done")):
    os.remove(f)
  for f in glob(os.path.join(args.work_dir, "*.makeflowlog")):
    os.remove(f)

# Create all XML and text files
for x in xmls:
  with open(os.path.join(args.work_dir, x), "w") as f:
    print("Writing XML collection %s" % x)
    f.write(xmls[x])
for t in txts:
  with open(os.path.join(args.work_dir, t), "w") as f:
    print("Writing text collection %s" % t)
    f.write(txts[t])

# Copy all input files once, when appropriate, to the work directory. Files accessed from a remote
# location will not be copied
for f in set([ i for j in jdls for k in jdls[j] for i in jdls[j].get("InputFile", []) ]):
  dest = os.path.join(args.work_dir, f)
  if f in xmls or f in txts or f.endswith(",late") or re.search("^(/|[^ :]+:)", f):
    continue  # do not copy this file (was generated, or from a remote location, or late creation)
  print("Copying %s to the work directory" % f)
  try:
    os.remove(dest)
  except:
    pass
  try:
    copy2(f, dest)  # try from current dir first
  except:
    try:
      copy2(os.path.join(lib_path, f), dest)  # fallback on installation path
    except IOError:
      print("Cannot copy input file \"%s\", please make it available in the "
            "current directory (%s) or remove it from the JDL" % (f, os.getcwd()))
      exit(1)

# Remove late creations from all input lists
for k in jdls:
  jdls[k]["InputFile"] = [ re.sub(",late$", "", x) for x in jdls[k].get("InputFile", []) ]

# Generate all run*.sh scripts (the job wrappers from Jinja)
for j in jdls:
  print("Writing job wrapper run%s.sh" % j)
  gen_runjob(os.path.join(args.work_dir, "run%s.sh" % j), jdls[j], args.dry_run)

# Touch donefiles to pretend we have completed the earlier stages
if args.start_at:
  for tier in stages:
    for to_touch in [ x for x in tier if x != args.start_at ]:
      print("Pretending we have executed step %s" % to_touch)
      jr = jdls[to_touch].get("JobRange")
      if jr:
        for jobindex in range(jr[0], jr[1]+1):
          open(os.path.join(args.work_dir, "%s%04d.done" % (to_touch, jobindex)), "w").close()
      else:
        open(os.path.join(args.work_dir, "%s.done" % to_touch), "w").close()
    if args.start_at in tier:
      break

# Run the workflow with additional checks: every Makeflow manifest is generated at each step
for step in steps:

  # Special instructions for certain steps: may affect Makeflow creation
  tbf = os.path.join(args.work_dir, "timeBins.log")
  if step == "CPass0":
    if os.path.isfile(tbf) and args.run and (not args.start_at or stages_flat.index(args.start_at) < stages_flat.index("cpass0SpDistMaps")):
      os.remove(tbf)
  elif step == "CPass0_SPC":
    print("CPass0_SPC: parsing time bins")
    if (not os.path.isfile(tbf) and not args.run) or (args.run and args.dry_run):
      print("CPass0_SPC: WARNING: could not find timeBins.log, creating a dummy one!")
      with open(tbf, "w") as fp:
        fp.write("0 0 0\n")
    with open(tbf) as fp:
      jdls["cpass0SpDistMaps"]["timeBins"] = [ x for x in fp.read().split("\n") if re.search("^[0-9]+ [0-9]+ [0-9]+$", x) ]
    if not jdls["cpass0SpDistMaps"]["timeBins"]:
      print("ERROR: CPass0 SP time bins creation returned no valid lines, check timeBins.log")
      exit(2)

    # Those are not real input files, but they are passed as "input file" parameter of each job in
    # order to make it possible to parse #alienfilename# in the OutputDir formation
    jdls["cpass0SpDistMaps"]["InputFilesFromList"] = [ x.replace(" ", "_") for x in jdls["cpass0SpDistMaps"]["timeBins"] ]
    jdls["cpass0SpDistMaps"]["JobRange"] = [ 0, len(jdls["cpass0SpDistMaps"]["timeBins"])-1 ]

    # Dynamically generate alitpcdcalibres.txt
    print("CPass0_SPC: generating collection of output files")
    pattern = preprocess_var(os.path.join(jdls["cpass0SpDistMaps"]["OutputDir"], "alitpcdcalibres.root"), sh_mode=False)
    with open(os.path.join(args.work_dir, "alitpcdcalibres.txt"), "w") as fp:
      fp.write( \
        "\n".join([ j2env.from_string(pattern).render(alien_counter=ji, input_files=jdls["cpass0SpDistMaps"]["InputFilesFromList"])
                    for ji in range(0, len(jdls["cpass0SpDistMaps"]["InputFilesFromList"])) ]) + "\n" )

    # Touch donefiles for cpass0SpDistMaps if appropriate
    if args.start_at and stages_flat.index(args.start_at) > stages_flat.index("cpass0SpDistMaps"):
      print("CPass0_SPC: touching done files")
      for i in range(0, len(jdls["cpass0SpDistMaps"]["timeBins"])):
        open(os.path.join(args.work_dir, "cpass0SpDistMaps%04d.done" % i), "w").close()

  # Recollection of validation reports: file list
  if step == "MCPass" or steps.index(step) >= steps.index("CPass0_SPC"):
    buf = ""
    for j in jdls:
      if j.startswith("mergeValidationReports"):
        continue
      if not "validation_report.txt" in jdls[j].get("Output", []):
        print("WARNING: no validation reporting in job %s" % j)
        continue
      if "JobRange" in jdls[j]:
        pattern = os.path.join(preprocess_var(jdls[j]["OutputDir"], sh_mode=False), "validation_report.txt")
        buf += "\n".join([ j2env.from_string(pattern).render(alien_counter=ji, input_files=jdls[j]["InputFilesFromList"])
                                      for ji in range(jdls[j]["JobRange"][0], jdls[j]["JobRange"][1]+1) ]) + "\n"
        del pattern
      else:
        buf += os.path.join(preprocess_var(jdls[j]["OutputDir"], sh_mode=False), "validation_report.txt") + "\n"
    with open(os.path.join(args.work_dir, "validation_report_list.txt"), "w") as fp:
      fp.write(buf)
    del buf

  # Remove validation report if appropriate
  vf = os.path.join(args.work_dir, "validation_report_full.txt")
  idx = max([ stages_flat.index(x) for x in stages_flat if x.startswith("mergeValidationReports") ])
  if os.path.isfile(vf) and args.run and (not args.start_at or stages_flat.index(args.start_at) <= idx):
    os.remove(vf)

  print("Generating Makeflow_%s" % step)
  makeflow = j2env.from_string("""
# Makeflow_{{step}} -- automatically generated

{% if step == "MCPass" %}
# sim
{% for jobindex in range(joba,jobb) -%}
{{ "sim"|job_head(index=jobindex) }}
{{ "sim"|job_cmd(index=jobindex) }}
{% endfor %}

# simResMerge
{{ "simResMerge"|job_head }}{% for j in range(joba,jobb) %}{{ " sim%04d.done"|format(j) }}{% endfor %}
{{ "simResMerge"|job_cmd }}

# simQaMerge
{{ "simQaMerge"|job_head }}{% for j in range(joba,jobb) %}{{ " sim%04d.done"|format(j) }}{% endfor %}
{{ "simQaMerge"|job_cmd }}

# simQaPlots
{{ "simQaPlots"|job_head }} simQaMerge.done
{{ "simQaPlots"|job_cmd }}

# mergeValidationReportsMC
validation_report_full.txt {{ "mergeValidationReportsMC"|job_head }} simQaPlots.done
{{ "mergeValidationReportsMC"|job_cmd }}
{% endif %}

{% if step == "CPass0" %}
# cpass0
{% for jobindex in range(joba,jobb) -%}
{{ "cpass0"|job_head(index=jobindex) }}
{{ "cpass0"|job_cmd(index=jobindex) }}
{% endfor %}

# cpass0Merge
{{ "cpass0Merge"|job_head }}{% for j in range(joba,jobb) %}{{ " cpass0%04d.done"|format(j) }}{% endfor %}
{{ "cpass0Merge"|job_cmd }}

# cpass0ResMerge
{{ "cpass0ResMerge"|job_head }}{% for j in range(joba,jobb) %}{{ " cpass0%04d.done"|format(j) }}{% endfor %}
{{ "cpass0ResMerge"|job_cmd }}

# cpass0SpBins
timeBins.log {{ "cpass0SpBins"|job_head }} cpass0Merge.done cpass0ResMerge.done
{{ "cpass0SpBins"|job_cmd }}
{% endif %}

{% if step == "CPass0_SPC" %}
# cpass0SpDistMaps
{% for jobindex in range(0,jdls["cpass0SpDistMaps"]["timeBins"]|length) %}
{{ "cpass0SpDistMaps"|job_head(index=jobindex) }} cpass0SpBins.done
{{ "cpass0SpDistMaps"|job_cmd(index=jobindex) }} {{ jdls["cpass0SpDistMaps"]["timeBins"][jobindex] }}
{% endfor %}

# cpass0SpCalib
{{ "cpass0SpCalib"|job_head }}{% for j in range(0,jdls["cpass0SpDistMaps"]["timeBins"]|length) %}{{ " cpass0SpDistMaps%04d.done"|format(j) }}{% endfor %}
{{ "cpass0SpCalib"|job_cmd }}
{% endif %}

{% if step == "CPass1" %}
# cpass1
{% for jobindex in range(joba,jobb) -%}
{{ "cpass1"|job_head(index=jobindex) }} cpass0SpCalib.done
{{ "cpass1"|job_cmd(index=jobindex) }}
{% endfor %}

# cpass1Merge
{{ "cpass1Merge"|job_head }}{% for j in range(joba,jobb) %}{{ " cpass1%04d.done"|format(j) }}{% endfor %}
{{ "cpass1Merge"|job_cmd }}

# cpass1ResMerge
{{ "cpass1ResMerge"|job_head }}{% for j in range(joba,jobb) %}{{ " cpass1%04d.done"|format(j) }}{% endfor %}
{{ "cpass1ResMerge"|job_cmd }}

# cpass1QaMerge
{{ "cpass1QaMerge"|job_head }} cpass1Merge.done
{{ "cpass1QaMerge"|job_cmd }}

# cpass1QaPlots
{{ "cpass1QaPlots"|job_head }} cpass1QaMerge.done
{{ "cpass1QaPlots"|job_cmd }}
{% endif %}

{% if step == "PPass" %}
# ppass
{% for jobindex in range(joba,jobb) -%}
{{ "ppass"|job_head(index=jobindex) }} cpass1Merge.done
{{ "ppass"|job_cmd(index=jobindex) }}
{% endfor %}

# ppassResMerge
{{ "ppassResMerge"|job_head }}{% for j in range(joba,jobb) %}{{ " ppass%04d.done"|format(j) }}{% endfor %}
{{ "ppassResMerge"|job_cmd }}

# ppassQaMerge
{{ "ppassQaMerge"|job_head }}{% for j in range(joba,jobb) %}{{ " ppass%04d.done"|format(j) }}{% endfor %}
{{ "ppassQaMerge"|job_cmd }}

# ppassQaPlots
{{ "ppassQaPlots"|job_head }} ppassQaMerge.done
{{ "ppassQaPlots"|job_cmd }}

# mergeValidationReportsReco
validation_report_full.txt {{ "mergeValidationReportsReco"|job_head }} ppassQaPlots.done
{{ "mergeValidationReportsReco"|job_cmd }}
{% endif %}
""")
  with open(os.path.join(args.work_dir, "Makeflow_"+step), "w") as mf:
    mf.write(makeflow.render(jdls=jdls,
                             step=step,
                             joba= jdl["JobRange"][0],
                             jobb= jdl["JobRange"][1]+1))

  if args.graph:
    print("Generating graph for step %s" % step)
    wd = os.getcwd()
    os.chdir(args.work_dir)
    try:
      subprocess.check_call("makeflow_viz Makeflow_%s | dot -T pdf -o Makeflow_%s.pdf" % (step, step),
                            shell=True)
    except:
      print("WARNING: cannot generate workflow graph for %s" % step)
    os.chdir(wd)

  # Run Makeflow in dryrun mode to pretend we have completed the earlier stages
  if args.start_at:
    print("Dry-running Makeflow on step %s" % step)
    wd = os.getcwd()
    os.chdir(args.work_dir)
    devnull = open(os.devnull)
    subprocess.check_call([ "makeflow", "-T", "dryrun", "Makeflow_"+step ],
                          stdout=devnull, stderr=devnull)
    os.chdir(wd)

  print("Executing step %s" % step)
  wd = os.getcwd()
  os.chdir(args.work_dir)
  try:
    subprocess.check_call( ["time"] + ([] if args.run else ["echo", "+"]) + \
                           [ "makeflow", "Makeflow_"+step ] + args.makeflow_opts)
  except subprocess.CalledProcessError as e:
    rv = e.returncode if e.returncode > 0 else 128-e.returncode
    print("Step %s finished with an error: %d" % (step, rv))
    exit(rv)
  os.chdir(wd)
  print("Step %s finished successfully" % step)

if args.run:
  vf = os.path.join(args.work_dir, "validation_report_full.txt")
  if os.path.isfile(vf):
    if os.stat(vf).st_size == 0:
      print("Validation report %s is empty: no problems occurred!" % vf)
      exit(0)
    else:
      print("ERROR: validation report %s is not empty: errors occurred" % vf)
      exit(1)
  else:
    print("Validation report not found: assuming success")
    exit(0)
exit(0)
